## 图神经网络论文精读：

### 一：预备知识：

**图（Gragh）是一种常见的数据结构，它可以由三部分表示：顶点，边，整体图（全局），并且他们可以用向量的形式表示出来。**

图在生活中无处不在，尤其是在近些年来图神经网络构建起来后，在药物，物理，车流量预测等方面起到很大的作用，并且目前发展前景非常好。

图还可以表示很多信息，图片，文本，任务社会关系等等都可以。

**在图层面，顶点层面，边层面都可以有相关问题。**

那么，我们发现对图存在很多问题，尤其是在如果我们使用邻接矩阵来存储的话，就会出现数据量过大的问题，但如果我们使用稀疏矩阵进行存储，那么就会在GPU加速的计算效率上存在问题。

**那么对于GNN图神经网络**

（插图1）

**GNN是对图上所有属性（点，边，全局）进行可以优化的变换，同时保持住图的对称信息。会对图中点，边等的向量进行变换，但不会对边连接的哪些顶点等信息不会改变。**

区别在于是先更新节点还是边。

### 二：摘要：

摘要：**基础模型**已经成为众多人工智能应用中的关键组件，并在自然语言处理等多个领域展示了显著的成功。同时，图机器学习领域也正从浅层方法向更复杂的深度学习方法转变。基础模型在**泛化和适应能力**方面的优势激发了图机器学习研究者探讨开发新图学习范式的潜力。这一范式设想在广泛的图数据上预训练模型，并能够适应各种图任务。尽管对此的兴趣日益增长，但关于这一新领域的明确定义和系统分析却明显不足。为此，本文引入了**图基础模型（Graph Foundation Models，GFMs）的概念**，并对其关键特征和基础技术进行了详尽的解释。我们将现有与GFMs相关的工作根据其对图神经网络和大型语言模型的依赖性分为三类。此外，本文还对GFMs的当前状态进行了全面的回顾，并展望了这一快速发展的领域未来可能的研究方向。

**相对于GNN，提出了图基础模型的概念**

### 三：结论：

**基础模型和图机器学习**的发展推动了一个新研究方向的出现，旨在在广泛的图数据上进行训练，并将其应用于各种下游图任务。在本文中，我们首次提出了图基础模型（Graph Foundation Models, GFMs）的概念，并介绍了相关的概念和代表性方法。我们将现有的GFMs相关工作根据其对**图神经网络（GNNs）和大型语言模型（LLMs）的依赖性**分为三类：**基于GNN的模型、基于LLM的模型以及GNN+LLM结合的模型**。对于每一类方法，我们分别介绍了它们的**骨干架构、预训练策略和适应策**略。在对图基础模型的现状进行全面概述后，本文还指出了该领域未来可能的发展方向。

**解读：根据前文所说的GNN，同时我们将它与LLM的依赖性分为三种的图基础模型，这一点也是可以类比之前在自然语言处理等领域产生的基础模型延申。**

### 四：介绍：

随着计算能力的提升和深度学习技术的突破，人工智能（AI）领域提出了“基础模型”的概念：**基础模型是指在广泛的数据上进行训练，并能适应各种下游任务的模型**【1】。基础模型具备独特的属性，如涌现性和同质化，使其能够成为众多下游AI应用的主要构建模块【1】。**涌现性表明，随着基础模型的规模扩大，它可能会自发地展现出新的能力**【2】。**同质化则指模型的多功能性，使其能够在各种应用中部署**【1】。借助大型语言模型（LLMs）的发展，基础模型的概念首先在自然语言处理（NLP）中得以实现。从那时起，基础模型展现了令人印象深刻的多样性，**不仅可以处理文本数据，还可以处理图像、视频、音频数据以及多模态输入**。这种多功能性使其在计算机视觉【3】、音频信号处理【4】以及推荐系统【5】等任务中表现出色。

**解读：现在对于基础模型的发展已经有了很大进展，尤其是在涌现性和同质化的两个特性程度上**

与NLP领域的演变类似，图机器学习也正在经历范式转变。在其早期阶段，图任务主要使用浅层方法，如**随机游走【6】和矩阵分解**【7】。然而，这些方法通常仅限于无属性图上的传导学习【8】。最近，向深度学习方法的转变催生了图神经网络（GNNs）的兴起。**GNNs通过引入信息传递机制，彻底改变了这一领域，节点通过迭代地从邻居节点中聚合信息**。在完全监督、半监督或无监督设置中，研究人员开发了多种定制的图模型。这些进展在节点分类【9】、链接预测【10】、图分类【11】和图聚类【12】等任务中带来了显著改进。然而，GNN模型仍然存在一些挑战。例如，GNN在**表达能力【13】和泛化能力【14】方面受到限制**，尤其是在数据集不断扩大和任务种类日益增加的情况下。

基础模型在不同领域的显著成功日益引起了图机器学习研究者的兴趣。这自然引发了一个问题：图基础模型是否可能成为图机器学习的下一个前沿？如果实现，这类模型将具有更强的表达能力、更好的迁移性，并能够应用于更复杂的图数据和任务。如图1所示，图基础模型（GFM）被设想为在广泛的图数据上预训练的模型，能够适应多种下游图任务。与传统基础模型类似，GFM也预期具备两个主要特征：涌现性和同质化。具体来说，涌现性指的是大规模图模型中独有的新能力，而同质化则表示模型在不同类型的图任务中的适应能力。现有的深度图学习方法难以具备这些特征：**它们的内在架构和学习范式专注于特定任务，限制了对大量未标记数据的利用，进而限制了它们的表达和泛化能力**。

**解读：类比于基础模型在NLP等多个领域的重大突破，我们尝试在图机器学习领域展开图基础模型，这样相较于GNN可以在表达能力和泛化能力上有很大突破！但是由于现有的深度图学习方法难以具备这些特征，他们在内在架构和学习范式上专注于特定的任务，也就限制了这两种能力。**

受到LLMs在NLP中作为基础模型的成功启发，研究人员开始探索图基础模型在涌现性和同质化能力方面的可能性。这些探索主要集中在GFMs的骨干架构设计和不同学习范式上，包括**预训练和适应**，因为这些是LLMs实现上述能力的关键策略。首先，基础模型的涌现能力通常仅存在于具有大量参数的骨干中，而图神经网络的参数数量显著少于语言模型的骨干。**这意味着图基础模型的骨干可能需要重新设计，以实现更大规模的知识存储，从而实现涌现性**。由于图数据通常与丰富的文本信息相关，另一种方法是将LLMs用作图基础模型。然而，LLMs是否能有效处理图数据及相关任务尚不确定，关键是要确定如何在LLMs中建模图结构。此外，基础模型的同质化要求以统一的方式处理各种任务。由于节点间的复杂关系、多种属性形式以及任务在节点、边和图层级的多样性，设计有效的预训练任务和下游任务适应方法对图数据来说具有挑战性。**因此，还需要设计合适的预训练任务和适应机制**。

尽管目前尚无关于图基础模型设计和实现的明确解决方案，本文对一些相关研究进行了调查，并根据它们对GNNs和LLMs的依赖性将其分为三种不同的方式：

**（1）基于GNN的模型：旨在通过在骨干、预训练和适应方面的创新来增强现有的图学习范式；**

**（2）基于LLM的模型：探索使用LLM作为图基础模型的可行性，通过将图转换为文本或标记；**

**（3）GNN+LLM结合的模型：探索GNN和LLM之间的多种协同方式，以增强其能力。**

据我们所知，这是关于图基础模型的首个综述。**现有的基础模型综述通常探讨语言和视觉等模态**【1, 15】，而不是图。此外，还有两篇关于知识图谱和大型语言模型的综述【16, 17】，但知识图谱由于其在构建和应用上的特殊性，不在本文讨论范围内。我们还注意到一篇最近的文章提到了大规模图模型的概念【18】，但它强调的是观点陈述，缺乏系统的分类。因此，本文的贡献总结如下：
- 本文首次定义了图基础模型的概念，并探讨了其能力的核心问题和特征。
- 本文引入了一个新的分类法，并讨论了每种图基础模型方法的优点和局限性。
- 本文为图基础模型的发展方向提供了有前景的建议。

本文的后续部分组织如下：在第2部分中，我们介绍了与图基础模型相关的背景知识。第3部分定义了图基础模型，并突出了它们与语言基础模型的相似点和不同点。第4至第6部分分别详细讨论了基于GNN的模型、基于LLM的模型以及GNN+LLM结合的模型作为图基础模型的相关工作。第7部分讨论了图基础模型的未来方向。第8部分总结了本文的关键点。

**解读：本篇综述的核心在于对图基础模型进行定义！讨论了图模型基础方法的优点和局限性。**

### 五：背景信息：

在介绍图基础模型的概念之前，本节将首先回顾一些相关背景知识，即深度图学习和语言基础模型。具体来说，我们将从三个方面介绍它们：数据、骨干架构和学习范式。

#### 5.1 深度图学习

许多现实世界的系统自然地以图的形式表示。深度图学习在多个领域中具有重要意义，因为它能够强大地建模和表示实体之间复杂的关系和交互。在本节中，我们将提供一个简明的概述，涵盖深度图学习的主要步骤，包括图数据、骨干架构和学习范式三个关键方面。

##### 5.1.1 图数据

图是一种多功能且强大的数据表示方式，能够捕捉网络中实体之间复杂的关系和依赖性。图数据具有以下几个特征：
1. **非欧几里得性质**：图数据本质上是非欧几里得的，因为它缺乏传统数据格式中固定的几何结构【19】。与具有固定邻域区域和确定顺序的欧几里得数据相比，**图数据明确编码了实体之间的复杂连接**。
2. **各种领域**：图数据在多个领域中普遍存在，包括社交网络【20】、生物学【21】和交通运输【22】等。不同领域的图数据可能表现出不同的特征，包括不同的节点类型、边的语义和结构模式。例如，在生物网络中，节点可以表示蛋白质、基因或代谢物，而边可能表示蛋白质-蛋白质相互作用或代谢反应。这种领域特定的变异性使得创建一个能够有效泛化和适应不同图结构的通用模型变得具有挑战性【23】。
3. **各种类型**：图数据有多种类型，包括同质图【24】、异质图【25】、超图【26】和动态图【27】等。同质图包含相同类型的节点和边，例如由论文组成的引文网络。异质图则包括多种类型的节点或边，例如包含作者和论文的引文网络。超图由连接多个节点的超边组成，能够建模节点之间的高阶关系。动态图则指节点和边随时间变化的图结构，例如由变化的交通流量形成的交通网络。

##### 5.1.2 骨干架构

作为当前主流的骨干架构，**图神经网络（GNN）已成为深度图学习的强大框架**。大多数GNN遵循消息传递框架【28】，该框架使得图中节点能够与其邻居交换信息**。例如，GCN【9】引入了图卷积层的概念，为许多后续GNN架构奠定了基础**。GraphSAGE【29】提出了一种通过归纳学习为大型图中的节点生成嵌入的方法。此外，GAT【30】将注意力机制引入GNN，使得节点在消息传递过程中能够为其邻居的权重分配重要性，从而增强了它们的表达能力。这些工作对GNN的发展做出了重大贡献，使其成为深度图学习的多功能工具。

尽管更深的神经网络可以实现更强的表达能力【31】，但加深GNN并不容易。其原因是，随着GNN层数的增加，**消息聚合过程中引入的信息过多，导致所有节点的表示变得相似【32】。这也被称为过度平滑问题**。此外，**层数的增加会导致感受野呈指数增长，从而引发过度压缩问题【13】，即大量信息被压缩到固定长度的节点向量中**。近年来，一些工作致力于解决深层图神经网络中的过度平滑问题和过度压缩问题，从而提高了下游任务的性能。例如，DropEdge【33】等创新方法通过随机删除边来增强GCN模型的性能和可扩展性（最多可达到64层）。另一个提高GNN表达能力的研究方向是图Transformer【34, 35, 36】。得益于其全连接的注意力机制和长程关系建模能力，图Transformer架构能够缓解过度平滑问题和过度压缩问题【37】。

##### 5.1.3 学习范式

深度图学习的学习范式主要包括三大类：监督学习、半监督学习和无监督学习。本节将对这些学习范式进行简要介绍。

**监督学习**：在监督学习环境中，算法利用包含输入数据和对应输出标签的训练数据集。这种范式在图分类【38】和图回归【39】等任务中有着实际应用。例如，在分子属性预测任务中【40】，通过使用带标签的训练数据训练GNN模型，能够预测分子的特定化学属性或特征，从而为药物开发和材料研究提供有价值的见解。

**半监督学习**：正如最近一项研究【41】所强调的，半监督学习构成了深度图学习的主要焦点。这种方法结合了带标签数据和未标记数据来提高模型性能，其中节点分类【9】成为一个突出的应用。**消息传递机制【28】使得GNN能够在邻近节点之间迭代交换信息**，从而能够在整个图中传播信息，有效地结合带标签和未标记数据以促进预测。此外，GNN还可以与传统方法（如标签传播）结合，以进一步提高其在半监督环境中的性能【42】。

**无监督学习**：无监督学习【43】是一种更广泛的机器学习方法，**旨在从数据中学习模式和结构**，而无需人工标注。例如，图聚类【44】旨在仅基于节点之间的关系和连接性发现图中的固有结构和模式。另一个例子是链接预测，其目标是预测缺失或即将发生的连接关系。无监督学习的一个重要子类是自监督学习，其目标是利用数据本身的内在信息生成标签【45】。**基于自监督学习，GNN可以进行端到端训练**，并应用于下游任务，如图聚类【12】和链接预测【10】。

#### 5.2 语言基础模型

AI领域目前正经历着一场标志性变革，这场变革的标志是一些特定的自然语言模型（如GPT-3）的出现，这些模型通过大规模自监督学习训练于大量和多样化的数据集。这些模型被称为基础模型，能够生成多种输出，使其能够处理广泛的下游任务。

与深度图学习流程不同，基础模型的方法采用预训练和适应框架，使得模型能够取得多项显著进展，包括“涌现”【2】和“同质化”【1】。基础模型主要在NLP领域中建立了自己，因此我们将在本节中讨论语言基础模型。

##### 5.2.1 语言数据

语言数据是指人类语言中的文本或口语内容，包括自然语言的语法规则及其相关的词语语义。它可以包括书面文件、转录的音频记录以及其他任何形式的语言通信。语言数据对于许多NLP任务，如机器翻译、情感分析和文本摘要等，都是至关重要的。研究人员和开发人员利用语言数据训练和评估语言模型及其他NLP算法。语言数据的质量和数量对NLP系统的性能起着至关重要的作用，影响其在各种语言任务中的准确性、鲁棒性和整体效果。与计算机视觉和其他领域相比，标注语言数据的规模相对较小，仅由几千个句子组成【46】。这一限制主要是由于手动标注的高成本。然而，互联网上、报纸和书籍等来源有大量的未标注语言数据可用，这为利用未标注数据进行模型预训练创造了机会**。此外，与图数据相比，语言数据作为欧几里得数据更容易建模，其丰富的语义信息显著增强了语言模型的知识可迁移性**。

##### 5.2.2 骨干架构

基础模型的一个早期突破是预训练语言模型（PLM），其设计旨在捕获上下文感知的词表示，这些表示作为通用语义特征证明了其极高的有效性。例如，**BERT【47】基于并行化的Transformer架构【48】和自注意力机制，通过在大量未标注数据上设计特定的预训练任务进行双向语言模型的预训练**。这项标志性研究显著提高了NLP任务的性能基准，并催生了大量后续研究，确立了当前流行的预训练和微调学习范式。

此外，研究人员发现，无论是通过增加模型规模还是训练数据规模，扩大PLM的规模通常会带来更强的下游任务能力。这些更大的PLM统称为大型语言模型（LLM），与其小型对应物（如1.5B参数的GPT-2和175B参数的GPT-3【49】）相比，这些模型表现出许多全新的涌现能力，能够从示例中学习新任务。尽管如此，最先进的LLM仍面临语法性、连贯性和常识性等方面的挑战。

##### 5.2.3：预训练：

**预训练与微调。** 在这种范式下，模型首先以一致的架构进行预训练，作为语言模型（LM），该模型会预测观察到的文本数据的概率。**相比于端到端训练，预训练具有显著的优势，并为基础模型的能力奠定了基础**。首先，在大量文本语料上进行预训练可以学习到通用的语言表示，这可能是解释模型涌现能力的原因之一【54】。此外，预训练提供了更好的模型初始化，通常能带来更好的泛化性能，从而实现多任务的同质化【54】。最后，预训练还起到了一种正则化的作用，有助于防止在较小数据集上过拟合【56】。例如，像GPT-3【50】这样的模型，使用语言建模目标进行训练，充分发挥了其在涌现能力和同质化方面的优势。

**在预训练阶段之后，基础模型获得了适用于广泛任务的通用能力**。然而，预训练模型仍然缺乏针对下游任务的特定信息，直接使用它们可能不会产生最佳效果。因此，我们需要对模型进行特定任务的微调，这就是所谓的微调。基于ULMFit【57】和BERT【47】等模型的成功，微调已成为适应预训练模型的主流方法。在这种框架下，主要的重点在于目标工程，涵盖了预训练和微调阶段的训练目标设计。例如，Pegasus【58】通过引入预测文档中重要句子的损失函数，改进了用于文本摘要的预训练模型。微调的优势在于可以在源任务和目标任务（或领域）之间转移知识，提升模型的性能。对于微调数据集相较于预训练数据集的较小规模，这一过程可以有效地进行适应，而不会丢失已存储的结构化语言知识。

**预训练、提示和预测。** 在这种范式下，与其调整预训练的语言模型以适应特定的下游任务，不如将下游任务重新构造成更接近原始语言模型训练时所处理的任务，这通过提供文本提示来实现。通过选择合适的提示，我们可以引导语言模型的行为，使其能够预测所需的输出，有时甚至不需要额外的特定任务的训练。这种方法的优势在于，通过一组合适的提示，可以使一个完全无监督的语言模型处理广泛的任务【55】。

从提示工程的角度来看，**创建合适提示的方法可以分为手动方法和自动方法**。手动方法包括基于人类的直觉创建直观的模板，这是最直接的提示制作方法。例如，影响深远的LAMA数据集【59】提供了手工设计的填空模板，用来评估语言模型的知识。然而，手动方法在成本和精度方面面临挑战。为了解决这些问题，一些方法开始尝试自动生成提示。例如，Prompt Mining【60】是一种模板发现方法，可以根据给定的训练输入和输出自主识别模板。

从另一个角度来看，根据模型和提示如何结合以生成结果，提示策略可以分为三种方法：无调优提示、提示调优和指令调优【5】。无调优提示直接基于提示生成答案，而不更改预训练大型语言模型的参数【53】【61】。提示调优在预训练模型的参数之外引入了额外的提示相关参数，并使用从下游训练样本中获得的监督信号更新这些额外的参数【62】【63】。指令调优则类似于微调的过程，调整语言模型的参数，同时引入固定的指令以引导模型的行为。这种方法在零样本场景中表现出潜在的改进【64】。

训练和适应的范式在表现上优于深度图学习，例如在少样本学习【65】中展示了其更强的表达能力和泛化能力。与旨在单个任务上取得更好性能的深度图学习不同，图模型基础模型被期望具备两个关键特征：**涌现性**和**同质化**。

**涌现性**：涌现性指的是，当图模型基础模型具有大量参数或在更多数据上进行训练时，模型会表现出一些新的能力，即涌现能力。受基础模型涌现能力【67, 68, 69】的启发，我们期望图模型基础模型也具有类似的能力，包括上下文学习、图推理和零样本图生成等。上下文学习可以在给定少量样本的情况下预测各种下游任务【70】。图推理通过基于图结构将复杂问题分解为多个子问题并逐步解决，例如解决图算法问题【71】。零样本图生成要求模型根据期望条件生成图而无需任何示例【72】。需要注意的是，尽管语言模型基础模型已经展示了各种涌现能力，但目前仅有少数工作【70, 71, 72】探讨了图模型基础模型的涌现能力。

**同质化**：同质化指的是图模型基础模型能够应用于不同格式的任务，例如节点分类、链接预测和图分类。需要注意的是，由于图任务与自然语言处理（NLP）任务的特性不同，实现同质化并非易事。实现同质化的根本问题在于确定如何统一不同类型的图任务。现有的工作尝试通过链接预测【65】或图级任务【66】来实现同质化，但哪种方法更优尚无共识。

#### 5.3.1关键技术

图模型基础模型主要包括两个关键技术：**预训练和适应**。本节将简要介绍这两项技术。

**预训练**：预训练在图模型基础模型的发展中起着至关重要的作用，类似于它在语言模型中的作用。它涉及在一个大型图数据集上**以自监督的方式对神经网络进行预训练。在预训练过程中，模型学习捕捉图中的结构信息、关系和模式。**对于图模型基础模型，有几种预训练策略。对比自监督学习【73, 74】通过对比正样本（例如，相似的节点对）与负样本（例如，不相似的节点对）来学习表示。生成式自监督学习【75, 76】鼓励模型重建图结构或预测原始图数据的特征。如果将大语言模型（LLM）作为图模型基础模型的一部分，还可以使用2.2.3节中介绍的预训练方法。这些多样化的预训练方法使图模型基础模型能够从原始图数据中学习有意义的表示，增强其在各种图任务中的泛化和适应能力。

**适应**：图模型基础模型的适应涉及根据特定下游任务或领域对模型进行调整，以提高其性能。这一过程包括几种技术，即**基础微调**、**参数高效微调**和**提示微调**。**基础微调（Vanilla FT）指对整个预训练模型在任务特定数据上进行训练，从而实现最高程度的定制化，但通常需要大量的数据和计算资源。参数高效微调（Parameter-efficient FT）【77, 78】则仅调整模型的一部分参数，在任务特定的适应和资源效率之间取得平衡**。提示微调【66, 79】是一种灵活的方法，它依赖外部提示来引导模型的行为，使其更加适应和有效。这些适应技术使图模型基础模型能够通过利用预训练的知识，并根据特定任务或领域定制其能力，在广泛的应用中表现出色。需要注意的是，尽管大语言模型已经发展出各种提示微调方法【55】，以及其他一些高效微调方法，如Prefix Tuning【62】，但针对图模型基础模型的提示微调方法相对较少。

#### 5.3.2 图数据的影响

基础模型的成功依赖于高质量的训练数据，基础模型在不同类型的测试数据上表现出显著差异。在本节中，我们将从图类型、图规模和图多样性三个方面讨论图数据对图模型基础模型的影响。

**图类型**：根据图中节点和边的类别数量，可以将图分为同质图和异质图。同质图中的所有节点和边都属于同一类别。例如，在社交图中，节点代表个人（用户），边代表好友关系，它是一个同质图，因为所有节点都是个人，所有边都代表好友关系。而异质图则具有多种类型的节点或边，代表不同类型的实体和关系【25】。例如，电子商务图可能包括用户、产品和购买关系的节点，构成一个异质图。对于图模型基础模型来说，处理异质图带来了更大的挑战，通常需要设计特定的主干架构和优化目标。然而，利用基于元路径的方法【80】，异质图可以映射为多个同质图，每个元路径对应一个同质图。例如，可以将训练好的同质图基础模型分别应用于这些元路径诱导的同质图中以获得节点嵌入。然后，可以将这些在不同元路径下的同质图嵌入结合起来。然而，除了同质图和异质图之外，现实世界中还存在一些更复杂的图类型，例如动态图和超图【81】，这些都给图模型基础模型带来了额外的挑战。

**图规模**：根据图中节点和边的数量，可以将图分为相对较小的图和大型图。小图规模较小，通常包含几十到几百个节点和边。例如，化学分子图表示小分子的结构，通常由几十到几百个原子组成。另一方面，大型图指的是具有大量节点和边的图，通常包含数百万甚至数十亿个节点和边。例如，阿里巴巴的电子商务图包括数十亿个节点和数百亿条边【82】。**对于图模型基础模型，大型图对其容量提出了更高的要求。首先，大型图由于其大量的节点和通常较稀疏的边，引入了更多的噪声，并在存储和计算方面带来了更大的挑战**【83】。此外，大型图通常表现出长程依赖关系【84】，这需要更多的神经网络层数和更高的参数数量，这加剧了基于图神经网络（GNN）模型的过平滑【32】和过压缩【13】问题。

**图多样性**：根据图数据集是否来自同一领域，可以将图分为同领域图和跨领域图。同领域图指来自相似或相关领域的图数据，通常包含相似类型的节点和边。例如，Facebook和微信的社交图来自相似的领域。而跨领域图【85】则涉及来自不同领域或数据源的图数据，通常包含不同类型的节点和边，旨在解决多领域问题或跨领域任务。例如，学术网络和分子图来自不同的领域。对于图模型基础模型来说，支持跨领域图带来了更大的挑战，因为来自不同领域的图缺乏统一的底层语义。这可能导致在应用模型于新数据集时出现较弱的迁移性能，甚至负迁移【86】。因此，解决不同领域的异质性并使同一图模型基础模型能够应用于不同领域的图是图模型基础模型面临的重大挑战。

#### 5.3.3 图任务的影响

语言模型基础模型可以广泛应用于各种NLP任务，而对于图模型基础模型，图任务的格式同样非常多样，可以分为三类：节点级任务、边级任务和图级任务。

**节点级任务**：节点级任务是对每个节点进行分类、回归或预测的任务。常见的节点级任务包括节点分类、节点回归和聚类系数预测。例如，在社交网络中，图节点可以代表用户，节点分类可以用于识别来自不同社交圈子或具有不同兴趣的用户。

**边级任务**：边级任务是对每条边进行分类、回归或预测的任务。常见的边级任务包括边分类、链接预测、最短路径预测、连通性预测和最大流预测。例如，在电子商务中，链接预测可以用于预测用户可能感兴趣的产品。

**图级任务**：图级任务关注整个图。常见的图级任务包括图分类、图回归、图生成、图聚类、图压缩和平均聚类系数预测。例如，在生物信息学中，图属性预测可以用于预测分子化合物的生物活性或毒性，从而加速药物发现过程。

总之，图任务的格式高度多样，可以分为三种类型：节点级、边级和图级，每种类型都有广泛的应用。这无疑增加

了设计图模型基础模型的难度。现有的图模型基础模型倾向于只关注其中一类图任务，例如在OGB-LSC挑战中【66】展示的基于分子的图模型基础模型，或者只关注图分类【86】。然而，针对所有图任务提出统一的图模型基础模型是未来研究的方向。

### 六：图模型基础模型与语言模型基础模型的异同

在这一节中，我们将从模型架构、预训练方法、适应技术和数据依赖性四个方面，比较图模型基础模型与语言模型基础模型之间的异同。

#### 3.2.1 模型架构

两者在模型架构上存在显著差异。**语言模型基础模型主要基于Transformer架构，这种架构擅长处理序列数据，并可以利用自注意力机制捕获长程依赖关系。而图模型基础模型则更多依赖于图神经网络（GNN），这类架构擅长在图结构中传播和聚合信息**。         

 然而，也有一些工作尝试将Transformer应用于图数据，取得了良好的效果【87】。

#### 3.2.2 预训练方法

在预训练方法上，**语言模型基础模型和图模型基础模型都采用了自监督学习**。然而，语言模型基础模型主要采用的是基于掩码语言建模（MLM）和因果语言建模（CLM）的策略，而图模型基础模型则更多依赖于对比学习【73, 74】和生成学习【75, 76】等方法。这是由于语言数据的线性结构与图数据的复杂结构之间的差异。

#### 3.2.3 适应技术

在适应技术上，语言模型基础模型的发展速度较快，**已经出现了多种微调方法，如基础微调、参数高效微调和提示微调等**。相比之下，图模型基础模型的适应技术仍在发展中，尽管已经有了类似的基础微调和参数高效微调方法，但提示微调的研究还较为有限【66】。

#### 3.2.4 数据依赖性

在数据依赖性方面，语言模型基础模型依赖于大量的文本数据，并且已经形成了诸如Common Crawl等大规模数据集。而图模型基础模型的发展则面临图数据的多样性和稀疏性问题，这使得构建高质量的预训练数据集变得更加困难。此外，图数据的异质性和大规模性也增加了图模型基础模型的训练难度。

总之，图模型基础模型和语言模型基础模型在架构、预训练、适应和数据依赖性等方面都有显著差异。然而，它们也有许多相似之处，**例如都通过预训练和适应技术来提高泛化能力，并在大规模数据上进行训练**。这些相似性和差异性为两者的结合和互补提供了可能性，也为未来的研究指明了方向。

-----

#### （待更新）
