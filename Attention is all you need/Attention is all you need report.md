## Attention is All you need report

### 论文链接：[Transfomer论文](https://arxiv.org/abs/1706.03762)

### 视频链接：[视频讲解](https://www.bilibili.com/video/BV1pu411o7BE/?spm_id_from=333.999.0.0&vd_source=6e22f74cbbb0cdf9444235d6ad11aabf)

### 一：论文精读与注释：

#### 一：摘要：

主流的序列转导模型【1】基于复杂的循环神经网络【2···】或卷积神经网络【3···】，这些网络包括编码器和解码器【4】。表现最好的模型还通过注意力机制【5···】连接编码器和解码器。我们提出了一种新的简单网络架构——Transformer，它完全基于注意力机制，完全摒弃了递归和卷积。对两个机器翻译任务的实验表明，这些模型在质量上更优，同时并行性更强，训练时间显著减少。我们的模型在WMT 2014英语到德语翻译任务上达到了28.4 BLEU分数，超过现有最佳结果（包括集成模型）2 BLEU以上。在WMT 2014英语到法语翻译任务中，我们的模型在8个GPU上训练3.5天后达到了新的单模型最优BLEU分数41.8，仅为文献中最佳模型训练成本的一小部分。我们还通过将其成功应用于英语成分解析任务，展示了Transformer在其他任务上的良好泛化能力，无论是大数据还是小数据训练。

**理解：主要讲述了Transformer作为一种全新的网络架构在机器翻译任务上表现了特别好的成果并且它完全基于注意力机制，相比于递归神经网络和卷积神经网络有很大的优势，尤其是在并行性和训练时间上。并且现在在其他NLP任务和图片视频等多个领域（多模态）表现出了很好的效果。**
**对于RNN这种处理时序序列的模型，当你的输入序列特别长的时候，由于并行性不好，计算所需时间就很长。**

【1】：**序列传导模型**：序列传导模型是处理输入序列并将其转换为输出序列的模型，常用于任务如机器翻译、语音识别和图像字幕生成。这些模型的主要目标是捕捉输入序列中的复杂依赖关系并生成相应的输出序列

【2···】：**RNN（循环神经网络）**：一种适合处理序列数据的神经网络。RNN通过循环连接来处理序列中的时间依赖关系，即它们在每个时间步的计算不仅依赖于当前输入，还依赖于前一个时间步的隐藏状态。常见的RNN变种包括长短期记忆网络（LSTM）和门控循环单元（GRU）

【3···】：**CNN（卷积神经网络）**：一种主要用于处理网格结构数据（如图像）的神经网络。CNN通过卷积操作来提取数据的局部特征。尽管CNN通常用于图像处理，它们也可以应用于序列数据，例如文本分类或时间序列分析。

【4】：**编码器-解码器架构**：序列传导模型中常见的结构，用于将输入序列映射到中间表示，再将其转换为输出序列

【5···】：**注意力机制**：注意力机制允许模型在处理序列数据时，有选择地关注输入序列的不同部分，从而更好地捕捉长距离依赖关系。

------

#### 二：结论：

我们提出了Transformer，一种全新的序列转导模型，完全基于注意力机制。通过摒弃递归和卷积操作，我们大大提高了模型的训练效率和并行性，同时在多个翻译任务上达到了最先进的性能。我们认为Transformer架构在许多其他序列建模任务中也具有广泛的应用前景。

**理解：Transformer在其他的方向上有更多的应用前景；**

------

#### 三：导言：

递归神经网络（RNN）、长短期记忆（LSTM）和门控递归单元（GRU）神经网络在序列建模和转导问题（如语言建模和机器翻译）中已被确立为最先进的方法。自此以来，许多努力继续推动递归语言模型和编码器-解码器架构的边界。

递归模型通常沿输入和输出序列的符号位置分解计算。将位置与计算时间步骤对齐，它们生成一系列隐藏状态ht，作为前一隐藏状态ht−1和位置t的输入的函数。**这种固有的顺序性在训练示例中限制了并行化**，这在较长的序列长度下变得至关重要，因为内存限制了跨示例的批处理。最近的工作通过因子化技巧【6】和条件计算【7】在计算效率上取得了显著改进，同时在后者的情况下也提高了模型性能。然而，顺序计算的基本限制仍然存在。

注意力机制已成为各种任务中引人注目的序列建模和转导模型的一个组成部分，允许在输入或输出序列中的任意位置之间建模依赖性。然而，除少数情况外，这些注意力机制通常与递归网络结合使用。

在这项工作中，我们提出了Transformer，一种摒弃递归，完全依赖注意力机制在输入和输出之间绘制全局依赖关系的模型架构。Transformer允许显著更多的并行化，并且在8个P100 GPU上训练仅12小时后就能达到新的翻译质量的最先进水平。

**理解：第一段话讲述了对于现阶段（2017）当时的主流方法是RNN的LSTM和GRU等，两个主流的模型一个是语言模型，另一个是编码器和解码器的结构。第二段是讲述RNN的特点，它的计算是把给定的序列从左往右一步步往前做，如果是一个句子就是一个词一个词往前看。RNN这样就可以把前面的信息放在t-1的隐藏状态下，在处理时序问题上有很好的效果。但同样的在并行性上能力低，导致在计算性能上比较差，时间需要比较长。同样的如果给定的序列比较长，就可能抛弃掉前面的一些信息，否则需要很大的ht，导致内存开销比较大。注意力机制已经在RNN上有所应用，那么现在提出了Transformer，完全基于注意力做出了更好的结果。**

【6】：**因子化技巧**：因子化技巧是一种通过将复杂的计算过程分解为更简单和更易计算的部分来优化模型的方法。因子化技巧在RNN中可以帮助减少计算量，降低内存需求，从而提升训练速度和效率。

1. **权重因子化（Weight Factorization）**：在RNN中，可以将大的权重矩阵分解为两个更小的矩阵。这种方法减少了参数的数量，从而加速计算并减小模型的内存占用。

   例如，将一个权重矩阵 W 分解为两个矩阵 W1 和 W2：
   
   $$ 
   W \approx W_1 \times W_2 
   $$

   通过这种分解，可以减少计算复杂度，提高训练效率。

3. **张量分解（Tensor Factorization）**：将高维张量分解为低维张量的乘积，从而减少计算量。这在处理高维输入数据时特别有效。

【7】：**条件计算**：是一种通过动态地选择哪些部分的网络进行计算来优化模型的方法。这种方法可以减少不必要的计算，从而提升效率。

1. **跳跃连接（Skip Connections）**：根据输入数据的特性，动态选择是否跳过某些层的计算。这种方法在深度网络中可以有效减轻梯度消失问题，并加速训练过程。
2. **门控机制（Gating Mechanisms）**：如LSTM和GRU中使用的门控机制，根据输入和隐藏状态的不同，动态选择哪些信息需要更新，哪些信息需要保留。这种方法使模型能够根据输入数据的不同，灵活地调整计算路径。
3. **稀疏激活（Sparse Activations）**：通过稀疏激活函数，只激活一部分神经元，从而减少计算量。比如，在每个时间步只激活输入数据最相关的部分。

------

#### 四：背景相关工作：

减少序列计算的目标也是扩展神经GPU（Extended Neural GPU）[16]、ByteNet [18] 和 ConvS2S [9]的基础，这些模型都使用卷积神经网络作为基本构建块，以便对所有输入和输出位置的隐藏表示进行并行计算。在这些模型中，两个任意输入或输出位置之间的信号关联所需的操作数随位置之间的距离增加而增长，对于ConvS2S是线性增长，而对于ByteNet是对数增长。这使得学习远距离位置之间的依赖关系变得更加困难[12]。在Transformer中，这种操作数减少到了一个常数，尽管代价是**由于平均注意力加权位置导致的有效分辨率降低**，这种影响我们通过第3.2节中描述的**多头注意力机制进行对抗**。

自注意力机制，有时称为内部注意力，是一种将单个序列的不同位置关联起来以计算该序列表示的注意力机制。自注意力机制已经成功应用于各种任务，包括阅读理解、抽象摘要、文本蕴含和学习任务无关的句子表示[4, 27, 28, 22]。

端到端记忆网络基于一种循环注意力机制，而不是基于序列对齐的递归机制，已被证明在简单语言问答和语言建模任务中表现良好[34]。

据我们所知，Transformer是第一个完全依赖自注意力机制来计算输入和输出表示的转换模型，**而没有使用序列对齐的RNN或卷积**。在接下来的章节中，我们将描述Transformer，阐述自注意力机制，并讨论其相对于[17, 18]和[9]等模型的优势

**理解：使用卷积神经网络，在对于比较长的序列难以建模，如果两个像素比较远，就需要很多层卷积才能实现，但是Transformer就可以做到一次性看到一层（扩大感受野），但是卷积神经网络的一个好处是它可以有多个输出通道，一个输出通道可以实现一个不同的模式，那么相应对于Transformer，我们提出了多头注意力机制。**

------

#### 五：模型架构：

大多数竞争性的神经序列转导模型具有编码器-解码器结构[5, 2, 35]。在这种结构中，编码器将符号表示的输入序列 (x1, ..., xn) 映射到一个连续表示序列 z = (z1, ..., zn)。给定 z，解码器然后逐个生成符号的输出序列 (y1, ..., ym)。在每一步，模型都是自回归的[10]，在生成下一个符号时使用前面生成的符号作为额外的输入。Transformer 遵循这种总体架构，对于编码器和解码器都使用堆叠的自注意力和逐点的全连接层，如图1的左右两半所示。

![Attention Image 1](images/Attention1.png)


**解读：对于编码器给定序列，会把原始输入转换为相应的向量表示，对于解码器会拿到编码器的输出一个长度的序列（可能不一样长），对于将输出再次作为输入，就是我们说的自回归，也就是RNN中的R，过去时刻的输出作为当前时刻的输入 **

#### 第一部分：编码器与解码器：

编码器：编码器由一个堆叠的N=6个相同的层组成。每一层都有两个子层。第一个子层是一个多头自注意力机制，第二个子层是一个简单的逐位置全连接前馈网络（MLP）【8】。我们在每个子层周围使用了残差连接【9】，然后进行层归一化[1]。也就是说，每个子层的输出是LayerNorm(x + Sublayer(x))，其中Sublayer(x)是子层本身实现的函数。为了方便这些残差连接，模型中的所有子层以及嵌入层都生成维度为d_model=512的输出。

解码器：解码器也由一个堆叠的N=6个相同的层组成。除了每个编码器层中的两个子层外，解码器还插入了一个第三子层，该子层对编码器堆栈的输出执行多头注意力。与编码器类似，我们在每个子层周围使用了残差连接，然后进行层归一化。我们还修改了解码器堆栈中的自注意力子层，以防止位置关注到后续位置。这种掩码，结合输出嵌入偏移一个位置的事实，确保了位置i的预测只能依赖于位置小于i的已知输出。

**解读：实际上就是两个参数可调（有多少层和维度）；**

**对比解读：Batchnorm（批量归一化）和layernorm（层归一化）： **

**Batchnorm：每一行是样本，每一列是特征（二维矩阵），每一次把每一个列（在minibatch里取同一特征实现均值为0方差为1标准化**

**layerNorm：对每个样本不同特征进行归一化。**

**但正常情况下输入是序列的样本，每个样本都有相应的小元素。转换为3维。但在持续序列里，样本的长度可能会发生变化，可能是长度特别长，也可能是短，那么这时如果按照batchnorm归一就会导致再遇见一个新的很长的样本，我们算总体的均值方差就效果不好，而如果选择layernorm我们就可以直接在单独的样本中算均值，相对稳定一些。**

**对于解码器这里，在注意力机制我们可以看到完整的输入，为了避免训练出现在解码器t时刻输出时借助t时刻之后输入进行预测（避免使用未来信息），所以使用带掩码的注意力机制**



#### 第二部分：注意力：

注意力函数可以描述为将一个查询和一组键-值对映射到一个输出，其中查询、键、值和输出都是向量。输出被计算为值的加权和，其中分配给每个值的权重是通过查询与相应键的兼容性函数计算的。

<img src="F:\科研文档\Transformer\Attention is all you need2.jpg" style="zoom:25%;" />

<img src="F:\科研文档\Transformer\Attention is all you need3.jpg" style="zoom:25%;" />

**解读：因为计算为值的加权和，这里的输出与值的维度相同**

【8】：MLP：是一种前馈神经网络，通常包括一个输入层、一个或多个隐藏层以及一个输出层。每一层都是由一组神经元组成，这些神经元通过权重和偏置相连。MLP的每个神经元接收前一层的输入，应用一个激活函数，然后将输出传递给下一层。常见的激活函数包括ReLU、sigmoid和tanh。

【9】：残差连接：是深度神经网络中的一种技巧，旨在解决深层网络中的梯度消失和梯度爆炸问题。残差连接通过在神经网络的某些层之间添加“快捷路径”或“跳跃连接”，允许梯度直接从后面的层传递到前面的层，从而使模型更容易训练。
$$
output = LayerNorm(x+Sublayer(x))
$$
通过这种方式，残差连接使得每一层仅需学习输入的残差，这比直接学习输出更加容易。



#### 缩放点积注意力：

我们将我们特定的注意力机制称为“缩放点积注意力”（图2）。输入由维度为 \(d_k\) 的查询和键，以及维度为 \(d_v\) 的值组成。我们计算查询与所有键的点积，每个点积除以 \(\sqrt{d_k}\)，然后应用softmax函数【10】以获得值的权重。

在实际操作中，我们同时对一组查询计算注意力函数，这些查询被打包成一个矩阵Q。键和值也被分别打包成矩阵K和V。我们计算输出矩阵如下：
$$
Attention(Q,K,V) = softmax(\frac{QK^T}{\sqrt{d_k}})V
$$
最常用的两种注意力函数是加性注意力[2]和点积（乘法）注意力。点积注意力与我们的算法相同，只是多了一个 \(\sqrt{d_k}\) 的缩放因子。加性注意力使用一个具有单一隐藏层的前馈网络来计算兼容性函数。虽然这两种方法在理论复杂度上类似，但在实际操作中，点积注意力更快且更节省空间，因为它可以使用高度优化的矩阵乘法代码实现。

对于小的 \(d_k\) 值，两种机制的表现相似，但对于较大的 \(d_k\) 值，未缩放的点积注意力表现不如加性注意力[3]。我们怀疑，对于较大的 \(d_k\) 值，点积的幅度会变得很大，从而将softmax函数推向梯度极小的区域。为了抵消这一影响，我们将点积缩放为 \(\frac{1}{\sqrt{d_k}}\)

**解读：常用的两种注意力函数中我们使用的是点积注意力函数，原理就是将QKV全部变为矩阵，然后通过Q与K做内积得到一个相似度，将算出的相似度放入softmax激活得到非负和为一的权重矩阵，再与V相乘得到我们的输出。（这种矩阵乘法就可以帮助我们并行计算每个元素）**

**这里我们所说的缩放指的是除以根号dk，因为在softmax函数中，如果dk比较大，做点积就会很极端（大或小）导致我们的值容易向两端0&1靠拢，计算梯度较大。所以我们除以了根号dk**

**mask主要是确保预测t时刻我只使用了1~t-1时刻的keyvaluepair进行运算。实际上是把后面部分设定为非常大的负数，以便于softmax做运算后得到几乎为0.**

【10】：softmax函数：是一种归一化函数，常用于分类任务的神经网络输出层。它将一个N维的向量转换为一个概率分布。Softmax函数的公式如下：
$$
softmax(z_i) = \frac{e^{z_i}}{\sum_{j = 1}^Ne^{z_j}}
$$
其中zi为输入向量中的第i个分量，N为向量的维度。

简单模型：

```python
import torch
import torch.nn as nn
import torch.nn.functional as F

class SimpleMLP(nn.Module):
    def __init__(self, input_size, hidden_size, output_size):
        super(SimpleMLP, self).__init__()
        self.hidden_size = hidden_size
        self.fc1 = nn.Linear(input_size, hidden_size)
        self.fc2 = nn.Linear(hidden_size, hidden_size)
        self.fc3 = nn.Linear(hidden_size, output_size)
    
    def forward(self, x):
        residual = x
        out = F.relu(self.fc1(x))
        out = self.fc2(out)
        out += residual  # 残差连接
        out = F.relu(out)
        out = self.fc3(out)
        out = F.softmax(out, dim=1)  # softmax函数
        return out

# 模型实例化
model = SimpleMLP(input_size=10, hidden_size=20, output_size=5)

# 输入数据
input_data = torch.randn(1, 10)
output = model(input_data)
print(output)

```



#### 多头注意力机制：

我们发现，与其对具有 dmodeld_{\text{model}}dmodel 维度的键、值和查询执行单个注意力函数，不如将查询、键和值通过不同的、学习到的线性投影分别线性投影 hhh 次到 dkd_kdk、 dkd_kdk 和 dvd_vdv 维度。然后，我们在这些投影后的查询、键和值的版本上并行执行注意力函数，产生 dvd_vdv 维度的输出值。将这些输出值拼接并再次投影，得到最终的值，如图2所示。

多头注意力允许模型在不同的位置从不同的表示子空间联合关注信息。单个注意力头的情况下，平均化会抑制这种能力。

.
$$
MultiHead = (Q,K,V) = Concat(head_1……head_h)W^O\\
head_i = Attention(QW_i^Q,KW_i^K,VW_i^V)
$$
在这项工作中，我们使用了 h=8个并行注意力层或头。对于每个头，我们使用 dk=dv=dmodel/h=64。由于每个头的维度减少，总的计算成本与全维度的单头注意力相似。

**解读：投影就是卷积操作，要做多头注意力机制是为了识别不一样的模式，对应不一样的计算办法。在这里就是先投影到低维，最后实现类似于多个输出通道的感觉。**

#### 注意力应用：

Transformer 在三种不同的方式中使用了多头注意力：

- 在“编码器-解码器注意力”层中，查询来自于上一层解码器，而记忆键和值来自于编码器的输出。这使得解码器中的每个位置都可以关注输入序列中的所有位置。这模拟了序列到序列模型中的典型编码器-解码器注意力机制，例如 [38, 2, 9]。
- 编码器包含自注意力层。在自注意力层中，所有的键、值和查询都来自同一个地方，在这种情况下，是编码器中上一层的输出。编码器中的每个位置都可以关注编码器中上一层的所有位置。
- 类似地，解码器中的自注意力层允许解码器中的每个位置关注解码器中该位置之前的所有位置。我们需要在解码器中防止左向信息流动以保持自回归属性。我们通过在缩放点积注意力中屏蔽掉 softmax 输入中的所有非法连接值（将其设为 -∞）来实现这一点。参见图2。

#### 第三部分：MLP：

除了注意力子层之外，我们的编码器和解码器中的每一层还包含一个全连接的前馈网络，该网络对每个位置单独且相同地应用。这个网络由两个线性变换和中间的 ReLU 激活函数组成。
$$
FFN(x) = max(0,xW_1+b_1)W_2+b_2
$$
虽然线性变换在不同位置上是相同的，但它们在每一层中使用不同的参数。另一种描述这种情况的方式是将其视为具有核大小为 1 的两个卷积。输入和输出的维度是 dmodel=512，内层的维度是 dff=2048。

**W1把x投影到2048，最后W2投影回512实质上就是把两个线性层重叠在一起。**

**对比：我们考虑最简单的状态，没有残差连接，只是单层的投影。**

单头Attention：我们知道给定的长为n的一些向量，经过Attention（对输入做加权和之后）得到同样长度一系列输出。加权和后进入positionwise的MLP，接下来对每一个输入的点做运算得到输出。所以Attention的作用实际上就是：**把整个序列里的信息抓取出来，做一次汇聚aggregation。**

RNN：同样的输入一系列长度为n的向量，首先经过一个简单的线性层，第一个得到一个，第二个就需要上一个的输出作为输入。

<img src="F:\科研文档\Transformer\微信图片_20240729155956.jpg" style="zoom:33%;" />

#### 第四部分：Embedding&softmax

与其他序列传导模型类似，我们使用学习到的嵌入将输入标记和输出标记转换为维度为 dmodel 的向量。我们还使用常见的学习到的线性变换和 softmax 函数将解码器的输出转换为预测的下一个标记的概率。在我们的模型中，我们在两个嵌入层和 pre-softmax 线性变换之间共享相同的权重矩阵，类似于 [30]。在嵌入层中，我们将这些权重乘以 根号dmodel

#### 第五部分：Positional Encoding：

由于我们的模型不包含循环和卷积，为了使模型利用序列的顺序，我们必须注入一些关于序列中标记相对或绝对位置的信息。为此，我们在编码器和解码器堆栈的底部将“位置编码”添加到输入嵌入中。位置编码与嵌入的维度 dmodeld_{\text{model}}dmodel 相同，以便两者可以相加。位置编码有许多选择，包括学习到的和固定的[9]。

在这项工作中，我们使用不同频率的正弦和余弦函数……

我们还尝试使用学习到的位置嵌入 [9]，发现这两个版本产生的结果几乎相同（见表3第(E)行）。我们选择正弦版本是因为它可能允许模型外推到训练期间遇到的序列长度更长的情况

**解读：Attention是不包含时序信息的，对于这里我们的输出只是value的权重和，他跟序列信息是无关的，意味着在处理时序数据时得出的数据Attention无法实现时序输出，所以我们在这里在输入这里加入了时序信息。**

#### 第六部分：为什么使用自注意力：

在本节中，我们将自注意力层与常用于将一个可变长度的符号表示序列（\(x_1, ..., x_n\)）映射到另一个相同长度序列（\(z_1, ..., z_n\)）的循环层和卷积层进行比较，其中
$$
x_i, z_i \in \mathbb{R}^d\
$$
如典型序列转换编码器或解码器中的隐藏层。为了证明我们使用自注意力的合理性，我们考虑了三个期望属性。

首先是每层的总计算复杂度。其次是可以并行化的计算量，以所需的最少顺序操作次数来衡量。第三是网络中长距离依赖关系的路径长度。学习长距离依赖关系是许多序列转换任务中的关键挑战。影响学习这种依赖关系能力的一个关键因素是网络中前向和后向信号必须遍历的路径长度。输入和输出序列中任意位置组合之间的路径越短，学习长距离依赖关系就越容易【12】。因此，我们还比较了由不同层类型组成的网络中任意两个输入和输出位置之间的最大路径长度。

如表1所示，自注意力层通过固定数量的顺序操作连接所有位置，而循环层需要 \(O(n)\) 的顺序操作。在计算复杂度方面，当序列长度 \(n\) 小于表示维度 \(d\) 时，自注意力层比循环层更快，这在当前最先进的机器翻译模型中使用的句子表示（如词片[38]和字节对[31]表示）中最常见。为了提高处理非常长序列任务的计算性能，自注意力可以限制为仅考虑以相应输出位置为中心的输入序列大小为 \(r\) 的邻域。这将使最大路径长度增加到 \(O(n/r)\)。我们计划在未来的工作中进一步研究这种方法。

单个卷积层的核宽度 \(k < n\) 时，并不能连接所有输入和输出位置对。在连续核的情况下，需要堆叠 \(O(n/k)\) 个卷积层，或在扩展卷积[18]的情况下需要 \(O(\log_k(n))\) 个卷积层，从而增加了网络中任意两个位置之间的最长路径长度。卷积层通常比循环层更昂贵，费用系数为 \(k\)。然而，可分离卷积【6】显著降低了复杂度，达到 
$$
O(k \cdot n \cdot d + n \cdot d^2)\
$$
即使 \(k = n\)，可分离卷积的复杂度也等于我们模型中使用的自注意力层和逐点前馈层的组合。

作为一个附带的好处，自注意力可以生成更具可解释性的模型。我们检查了模型中的注意力分布，并在附录中展示和讨论了示例。不仅各个注意力头明显学会了执行不同的任务，许多还表现出与句子的句法和语义结构相关的行为

### 三：简单代码示例：

（省略细节与优化）

```python
import tensorflow as tf
from tensorflow.keras.layers import Layer, Dense, Embedding, MultiHeadAttention, Dropout, LayerNormalization
from tensorflow.keras import Model

class TransformerEncoder(Layer):
    def __init__(self, num_heads, d_model, dff, rate=0.1):
        super(TransformerEncoder, self).__init__()
        self.mha = MultiHeadAttention(num_heads=num_heads, key_dim=d_model)
        self.ffn = tf.keras.Sequential([
            Dense(dff, activation='relu'),
            Dense(d_model)
        ])
        self.layernorm1 = LayerNormalization(epsilon=1e-6)
        self.layernorm2 = LayerNormalization(epsilon=1e-6)
        self.dropout1 = Dropout(rate)
        self.dropout2 = Dropout(rate)
        
    def call(self, x, training, mask):
        attn_output = self.mha(x, x, x, attention_mask=mask)
        attn_output = self.dropout1(attn_output, training=training)
        out1 = self.layernorm1(x + attn_output)
        
        ffn_output = self.ffn(out1)
        ffn_output = self.dropout2(ffn_output, training=training)
        out2 = self.layernorm2(out1 + ffn_output)
        
        return out2

class TransformerDecoder(Layer):
    def __init__(self, num_heads, d_model, dff, rate=0.1):
        super(TransformerDecoder, self).__init__()
        self.mha1 = MultiHeadAttention(num_heads=num_heads, key_dim=d_model)
        self.mha2 = MultiHeadAttention(num_heads=num_heads, key_dim=d_model)
        self.ffn = tf.keras.Sequential([
            Dense(dff, activation='relu'),
            Dense(d_model)
        ])
        self.layernorm1 = LayerNormalization(epsilon=1e-6)
        self.layernorm2 = LayerNormalization(epsilon=1e-6)
        self.layernorm3 = LayerNormalization(epsilon=1e-6)
        self.dropout1 = Dropout(rate)
        self.dropout2 = Dropout(rate)
        self.dropout3 = Dropout(rate)
        
    def call(self, x, enc_output, training, look_ahead_mask, padding_mask):
        attn1 = self.mha1(x, x, x, attention_mask=look_ahead_mask)
        attn1 = self.dropout1(attn1, training=training)
        out1 = self.layernorm1(x + attn1)
        
        attn2 = self.mha2(out1, enc_output, enc_output, attention_mask=padding_mask)
        attn2 = self.dropout2(attn2, training=training)
        out2 = self.layernorm2(out1 + attn2)
        
        ffn_output = self.ffn(out2)
        ffn_output = self.dropout3(ffn_output, training=training)
        out3 = self.layernorm3(out2 + ffn_output)
        
        return out3

class Transformer(Model):
    def __init__(self, num_heads, d_model, num_classes, dff, max_seq_len, rate=0.1):
        super(Transformer, self).__init__()
        self.encoder = TransformerEncoder(num_heads, d_model, dff, rate)
        self.decoder = TransformerDecoder(num_heads, d_model, dff, rate)
        self.embedding = Embedding(input_dim=10000, output_dim=d_model)
        self.final_layer = Dense(num_classes)
        
    def call(self, inputs, training, look_ahead_mask=None, padding_mask=None):
        x = self.embedding(inputs)
        enc_output = self.encoder(x, training, padding_mask)
        dec_output = self.decoder(x, enc_output, training, look_ahead_mask, padding_mask)
        final_output = self.final_layer(dec_output)
        return final_output

# Example usage
num_heads = 8
d_model = 128
num_classes = 10
dff = 512
max_seq_len = 100

model = Transformer(num_heads, d_model, num_classes, dff, max_seq_len)

```

### 四：补充知识学习

#### 一：CNN（卷积神经网络）

#### 原文链接：[CNN](https://adeshpande3.github.io/A-Beginner%27s-Guide-To-Understanding-Convolutional-Neural-Networks/)

https://adeshpande3.github.io/A-Beginner%27s-Guide-To-Understanding-Convolutional-Neural-Networks/

卷积神经网络。听起来像是生物学和数学的奇怪结合，并加入了一点计算机科学，但这些网络一直是计算机视觉领域最有影响力的创新之一。

#### 1.输入与输出：

当计算机看到图像（将图像作为输入）时，它将看到一个像素值数组。根据图像的分辨率和大小，它将看到一个 32 x 32 x 3 的数字数组（3 是指 RGB 值）。只是为了说明这一点，假设我们有一个 JPG 形式的彩色图像，其大小为 480 x 480。代表性阵列为 480 x 480 x 3。这些数字中的每一个都有一个从 0 到 255 的值，用于描述该点的像素强度。虽然这些数字在我们进行图像分类时毫无意义，但却是计算机唯一可用的输入。这个想法是，你给计算机这个数字数组，它将输出描述图像属于某个类别的概率的数字（猫为.80，狗为.15，鸟为.05等）

![img](https://adeshpande3.github.io/assets/Corgi3.png)

**对 CNN 功能的更详细概述是，您获取图像，将其传递到一系列卷积、非线性、池化（下采样）和全连接层，然后获得输出。正如我们之前所说，输出可以是单个类，也可以是最能描述图像的类的概率。现在，困难的部分是理解这些层中的每一层的作用**

#### **第一层 – 数学部分**

CNN 中的第一层始终是**卷积层**。首先要确保你记住的是这个conv（我将经常使用这个缩写）层的输入是什么。正如我们之前提到的，输入是一个 32 x 32 x 3 的像素值数组。现在，解释 conv 层的最佳方法是想象一个手电筒照在图像的左上角。假设这款手电筒照射的光线覆盖 5 x 5 的区域。现在，让我们想象一下这个手电筒在输入图像的所有区域滑动。在机器学习术语中，这种手电筒被称为**过滤器**（或有时称为**神经元**或**内核**），它照射的区域称为**感受野**。现在，这个过滤器也是一个数字数组（这些数字称为**权重**或**参数**）。一个非常重要的注意事项是，此滤波器的深度必须与输入的深度相同（这样可以确保数学运算有效），因此该滤波器的尺寸为 5 x 5 x 3。现在，让我们以过滤器所处的第一个位置为例。它将是左上角。当滤光片围绕输入图像滑动或**卷积**时，它会将滤光片中的值与图像的原始像素值相乘（也称为计算**元素乘法**）。这些乘法都相加了（从数学上讲，这总共是 75 次乘法）。所以现在你只有一个数字。请记住，此数字仅代表滤镜位于图像的左上角。现在，我们对输入音量上的每个位置重复此过程。（下一步是将滤波器向右移动 1 个单位，然后再次向右移动 1 个单位，依此类推）。输入体积上每个唯一位置都会产生一个数字。将过滤器滑过所有位置后，您会发现剩下的是一个 28 x 28 x 1 的数字数组，我们称之为**激活图**或**特征图**。获得 28 x 28 数组的原因是 5 x 5 滤镜可以在 32 x 32 输入图像上放置 784 个不同的位置。这 784 个数字映射到 28 x 28 数组

<img src="https://adeshpande3.github.io/assets/ActivationMap.png" alt="img" style="zoom: 80%;" />

**设现在我们使用两个 5 x 5 x 3 过滤器而不是一个。那么我们的输出体积将是 28 x 28 x 2。通过使用更多的过滤器，我们能够更好地保留空间维度。从数学上讲，这就是卷积层中发生的事情**

#### 第二层：高级层面：

但是，让我们从高层次来谈谈这种卷积实际上在做什么。这些筛选器中的每一个都可以被视为**功能标识符**。当我说特征时，我说的是直边、简单的颜色和曲线之类的东西。想想所有图像彼此之间具有的最简单的特征。假设我们的第一个滤波器是 7 x 7 x 3，它将是一个曲线检测器。（在本节中，为了简单起见，我们忽略了滤光片深度为 3 个单位的事实，只考虑滤光片的顶部深度切片和图像。作为曲线检测器，滤波器将具有像素结构，其中沿曲线形状的区域将有更高的数值（请记住，我们谈论的这些滤波器只是数字！）

![img](https://adeshpande3.github.io/assets/Filter.png)

现在，让我们回过头来用数学方式可视化这一点。当我们将此滤波器放在输入体积的左上角时，它正在计算滤波器和该区域的像素值之间的乘法。现在，让我们以一个我们想要分类的图像为例，让我们将过滤器放在左上角。

![img](https://adeshpande3.github.io/assets/OriginalAndFilter.png)

**请记住，我们要做的是将过滤器中的值乘以图像的原始像素值**

![img](https://adeshpande3.github.io/assets/FirstPixelMulitiplication.png)

基本上，在输入图像中，如果有一个形状通常类似于此滤波器所表示的曲线，那么所有乘法相加将产生一个大值！现在让我们看看当我们移动过滤器时会发生什么

![img](https://adeshpande3.github.io/assets/SecondMultiplication.png)

价值要低得多！这是因为图像部分没有任何内容对曲线检测器滤波器做出响应。请记住，此 conv 层的输出是一个激活映射。因此，在一个滤波器卷积的简单情况下（如果该滤波器是一个曲线检测器），激活图将显示图片中最有可能存在曲线的区域。在此示例中，我们的 26 x 26 x 1 激活映射的左上角值（26，因为使用 7x7 过滤器而不是 5x5）将为 6600。这个高值意味着输入音量中可能存在某种曲线，导致滤波器激活。我们的激活图中右上角的值将为 0，因为输入图片中没有任何东西导致过滤器激活（或者更简单地说，原始图像的那个区域没有曲线）。请记住，这只是一个过滤器。这只是一个过滤器，用于检测向外和向右弯曲的线。我们可以为向左弯曲的线或直边设置其他过滤器。过滤器越多，激活映射的深度就越大，我们掌握的有关输入量的信息就越多

#### 2.更深入理解：

现在，在传统的卷积神经网络架构中，这些卷积层之间散布着其他层。我强烈鼓励那些有兴趣的人阅读它们并理解它们的功能和效果，但从一般意义上讲，它们提供了非线性和维度保持，有助于提高网络的鲁棒性和控制过拟合。经典的 CNN 架构看起来像这样。

![img](https://adeshpande3.github.io/assets/Table.png)

最后一层非常重要，我们稍后会详细讨论。现在，让我们回顾一下到目前为止所学的内容。我们讨论了第一层卷积层中的滤波器设计目的——检测低级特征，如边缘和曲线。可以想象，要预测图像是否属于某一类别，我们需要网络能够识别更高级的特征，如手、爪子或耳朵。那么，让我们思考一下第一层卷积层之后的网络输出是什么。假设我们使用三个5 x 5 x 3的滤波器，输出将是一个28 x 28 x 3的体积。当通过另一层卷积层时，第一层卷积层的输出成为第二层卷积层的输入。这一点稍微有些难以可视化。当我们讨论第一层时，输入只是原始图像。然而，当我们讨论第二层卷积层时，输入是第一层卷积层生成的激活图。因此，输入的每一层基本上描述了原始图像中某些低级特征出现的位置。当你在此基础上应用一组滤波器（通过第二层卷积层）时，输出将是表示更高级特征的激活图。这些特征类型可能是半圆（曲线和直边的组合）或正方形（几条直边的组合）。随着网络的深入，通过更多的卷积层，你得到的激活图代表越来越复杂的特征。到网络末端，你可能会有一些滤波器在图像中出现手写时激活，有些滤波器在看到粉色物体时激活。如果你想了解更多关于卷积网络中滤波器可视化的信息，Matt Zeiler和Rob Fergus有一篇优秀的研究论文讨论这个话题。Jason Yosinski也在YouTube上有一个提供了很好的视觉表示的视频。另一个有趣的现象是，随着网络的加深，滤波器的感受野变得越来越大，这意味着它们能够考虑来自原始输入体积中更大区域的信息（换句话说，它们对更大范围的像素空间更敏感）。

#### 3.全连接层：

既然我们能够检测到这些高级特征，那么在网络末端添加一个全连接层就是锦上添花。这一层基本上会接收一个输入体积（无论是前一层卷积层、ReLU层还是池化层的输出），并输出一个N维向量，其中N是程序需要选择的类别数。例如，如果你想要一个数字分类程序，N将是10，因为有10个数字。这个N维向量中的每个数字代表某个类别的概率。例如，如果数字分类程序的结果向量是[0 .1 .1 .75 0 0 0 0 0 .05]，则表示图像是1的概率为10%，是2的概率为10%，是3的概率为75%，是9的概率为5%（旁注：还有其他方法可以表示输出，但我这里只展示softmax方法）。

全连接层的工作原理是它查看前一层的输出（我们记得应该代表高级特征的激活图），并确定哪些特征最与特定类别相关。例如，如果程序预测某个图像是一只狗，那么代表高级特征如爪子或四条腿的激活图将具有较高的值。同样，如果程序预测某个图像是一只鸟，那么代表高级特征如翅膀或喙的激活图将具有较高的值。基本上，全连接层查看哪些高级特征最强烈地与特定类别相关，并具有特定的权重，以便在计算权重与前一层的乘积时，得到不同类别的正确概率。

#### 4.Relu层：

ReLU层，即Rectified Linear Unit层，是神经网络中的一种激活函数层。ReLU的作用是对输入的每个元素进行非线性变换，将负值全部转换为零，而正值保持不变。具体的数学表示是：
$$
Relu(x) = max(0,x)
$$


1. **引入非线性**：神经网络需要非线性的激活函数来学习复杂的数据模式。ReLU通过将负值截断为零，引入了非线性，使得网络能够更好地拟合复杂的模型。
2. **避免梯度消失**：在使用sigmoid或tanh等激活函数时，随着网络层数的增加，梯度可能会逐渐消失，导致训练变得困难。ReLU由于其梯度为常数1（正区间），能够有效地缓解梯度消失的问题，从而加速训练过程。
3. **计算效率高**：ReLU的计算非常简单，只需要比较并截断负值，这使得它在计算上非常高效，适合大规模神经网络的训练。

#### 5.训练：

现在，我要讲的是神经网络中最重要的部分之一，即反向传播（backpropagation）。在阅读过程中，你可能会有很多问题，比如第一层卷积层中的滤波器是如何知道要寻找边缘和曲线的？全连接层是如何知道要查看哪些激活图的？每一层中的滤波器值是如何确定的？计算机通过一种叫做反向传播的训练过程来调整其滤波器值（或权重）。

### 神经网络的基础

在讨论反向传播之前，我们先回顾一下神经网络需要什么才能工作。在我们刚出生的时候，我们的头脑是一片空白，不知道猫、狗或鸟是什么。同样，在卷积神经网络（CNN）开始工作之前，权重或滤波器值是随机初始化的。滤波器不知道要寻找边缘和曲线，高层滤波器也不知道要寻找爪子和鸟喙。然而，随着我们长大，父母和老师向我们展示了不同的图片和图像，并给出了相应的标签。CNN的训练过程也是如此。假设我们有一个训练集，包含成千上万张狗、猫和鸟的图片，每张图片都有一个对应的标签。

### 反向传播的四个步骤

反向传播可以分为四个不同的部分：前向传播（forward pass）、损失函数（loss function）、反向传播（backward pass）和权重更新（weight update）。

1. **前向传播（Forward Pass）**：在前向传播过程中，你将一个训练图像（例如一个32 x 32 x 3的数组）输入到整个网络中。在第一次训练时，由于所有的权重或滤波器值都是随机初始化的，输出可能是类似于[.1 .1 .1 .1 .1 .1 .1 .1 .1 .1]的结果，即输出没有特别偏向任何一个数字。此时，网络还不能识别那些低级特征，因此无法做出任何合理的分类结论。

2. **损失函数（Loss Function）**：记住，我们现在使用的是训练数据，这些数据有图像和标签。假设第一个输入的训练图像是数字3，那么这个图像的标签是[0 0 0 1 0 0 0 0 0 0]。损失函数可以有很多种定义方式，但常见的一种是均方误差（MSE），其计算公式为：½ * (实际值 - 预测值)²。

   ![img](https://adeshpande3.github.io/assets/Equation.png)

3. **反向传播（Backward Pass）**：我们希望预测的标签（ConvNet的输出）与训练标签一致（即网络正确地做出了预测）。为了实现这一点，我们需要最小化损失。直观地看，这就像一个优化问题，我们需要找出哪些输入（在我们的情况下是权重）最直接地导致了网络的损失。这相当于计算损失L对权重W的导数，即dL/dW。通过反向传播，我们可以确定哪些权重对损失贡献最大，并找到调整它们的方法，使得损失减少。

4. **权重更新（Weight Update）**：在计算出梯度后，我们进行最后一步——权重更新。这一步骤中，我们调整滤波器的权重，使其朝梯度的相反方向变化。学习率是由程序员选择的参数。较高的学习率意味着权重更新时步伐较大，因此模型可能更快收敛到一个最优权重集。但学习率过高可能导致步伐过大，无法精确地达到最优点。

5. **学习率**是程序员选择的参数。高学习率意味着在权重更新中采取了更大的步骤，因此，模型收敛到一组最佳权重可能需要更少的时间。但是，学习率过高可能会导致跳跃太大，并且不够精确，无法达到最佳点

6. ![img](https://adeshpande3.github.io/assets/Weight.png)

前向传播、损失函数、反向传播和参数更新的过程就是一次训练迭代。程序会对每组训练图像（通常称为一个batch）重复这个过程固定的次数。在完成最后一个训练实例的参数更新后，希望网络已经被训练得足够好，各层的权重已正确调整。



#### 二：RNN（循环神经网路）(LSTM)

#### [博客地址](https://colah.github.io/posts/2015-08-Understanding-LSTMs/)

https://colah.github.io/posts/2015-08-Understanding-LSTMs/

人类不会每一秒都从头开始思考。当你阅读这篇文章时，你会根据你对前面单词的理解来理解每个单词。你不要把所有东西都扔掉，然后重新开始思考。你的思想有毅力。

传统的神经网络无法做到这一点，这似乎是一个主要缺点。例如，想象一下，您想要对电影中每个点上发生的事件类型进行分类。目前尚不清楚传统的神经网络如何利用其对电影中先前事件的推理来为后来的事件提供信息。

递归神经网络解决了这个问题。它们是带有循环的网络，允许信息持续存在。

<img src="https://colah.github.io/posts/2015-08-Understanding-LSTMs/img/RNN-rolled.png" alt="img" style="zoom:25%;" />

在上图中，有一大块神经网络，*一个*一个，查看一些输入xt并输出一个值**ht.循环允许信息从网络的一个步骤传递到下一个步骤。

这些循环使递归神经网络看起来有点神秘。然而，如果你仔细想想，就会发现它们与普通的神经网络并没有太大的不同。递归神经网络可以被认为是同一网络的多个副本，每个副本将消息传递给后继者。考虑一下如果我们展开循环会发生什么：

<img src="https://colah.github.io/posts/2015-08-Understanding-LSTMs/img/RNN-unrolled.png" alt="An unrolled recurrent neural network." style="zoom:25%;" />

这种类似链的性质揭示了递归神经网络与序列和列表密切相关。它们是用于此类数据的神经网络的自然架构。

它们肯定被使用了！在过去的几年里，将 RNN 应用于各种问题取得了令人难以置信的成功：语音识别、语言建模、翻译、图像标题......这样的例子不胜枚举

这些成功的关键是使用“LSTM”，这是一种非常特殊的递归神经网络，对于许多任务，它比标准版本要好得多。几乎所有基于递归神经网络的激动人心的结果都是通过它们实现的。本文将探讨的正是这些 LSTM。

#### 长期依赖问题：

RNN的吸引力之一是它们可能能够将先前的信息与当前任务联系起来，例如使用以前的视频帧可能会告知对当前帧的理解。如果 RNN 能够做到这一点，它们将非常有用。但是他们可以吗？这要视情况而定。

有时，我们只需要查看最近的信息来执行当前任务。例如，考虑一个语言模型，它试图根据前一个单词预测下一个单词。如果我们试图预测“云在*天空*中”中的最后一个词，我们不需要任何进一步的上下文——很明显，下一个词将是天空。在这种情况下，当相关信息与需要它的地方之间的差距很小时，RNN可以学习使用过去的信息。

<img src="https://colah.github.io/posts/2015-08-Understanding-LSTMs/img/RNN-shorttermdepdencies.png" alt="img" style="zoom:25%;" />

但在某些情况下，我们需要更多的背景信息。不妨试着预测经文中的最后一个字：“我在法国长大......我能说一口流利*的法语*。最近的信息表明，下一个词可能是一种语言的名称，但如果我们想缩小哪种语言的范围，我们需要更远的法国语境。相关信息与需要信息的点之间的差距完全有可能变得非常大。

不幸的是，随着这种差距的扩大，RNN变得无法学习连接信息。

<img src="https://colah.github.io/posts/2015-08-Understanding-LSTMs/img/RNN-longtermdependencies.png" alt="Neural networks struggle with long term dependencies." style="zoom:25%;" />

而LSTM的出现解决了这种长期依赖型的问题：

#### LSTM网络：

长短期记忆网络（通常简称为“LSTM”）是一种特殊的RNN，能够学习长期依赖性。它们是由[Hochreiter&Schmidhuber（1997）](http://www.bioinf.jku.at/publications/older/2604.pdf)引入的，并在后续工作中被许多人提炼和推广。[1](https://colah.github.io/posts/2015-08-Understanding-LSTMs/#fn1)它们在处理各种问题时效果非常好，现在被广泛使用。

LSTM 被明确设计为避免长期依赖性问题。长时间记住信息实际上是他们的默认行为，而不是他们努力学习的东西！

所有递归神经网络都具有神经网络的重复模块链的形式。在标准 RNN 中，这个重复模块将具有非常简单的结构，例如单个 tanh 层。

<img src="https://colah.github.io/posts/2015-08-Understanding-LSTMs/img/LSTM3-SimpleRNN.png" alt="img" style="zoom:25%;" />

LSTM 也具有这种链状结构，但重复模块具有不同的结构。没有单一的神经网络层，而是有四个，以一种非常特殊的方式进行交互。

<img src="https://colah.github.io/posts/2015-08-Understanding-LSTMs/img/LSTM3-chain.png" alt="A LSTM neural network." style="zoom:25%;" />

<img src="https://colah.github.io/posts/2015-08-Understanding-LSTMs/img/LSTM2-notation.png" alt="img" style="zoom: 50%;" />

#### LSTM背后的核心思想：

LSTM 的关键是单元状态，即贯穿图顶部的水平线。

细胞状态有点像传送带。它直接贯穿整个链条，只有一些微小的线性相互作用。信息很容易沿着它原封不动地流动。

![img](https://colah.github.io/posts/2015-08-Understanding-LSTMs/img/LSTM3-C-line.png)

LSTM确实具有删除或向细胞状态添加信息的能力，这些信息由称为门的结构仔细调节。

闸门是一种可以选择性地让信息通过的方式。它们由 sigmoid 神经网络层和逐点乘法运算组成。

<img src="https://colah.github.io/posts/2015-08-Understanding-LSTMs/img/LSTM3-gate.png" alt="img" style="zoom: 33%;" />

Sigmoid 层输出 0 到 1 之间的数字，描述每个组件应允许多少通过。值为 0 表示“不让任何事情通过”，而值为 1 表示“让所有东西通过！

LSTM 有三个这样的门，用于保护和控制单元状态。

#### 分步解读：

##### （遗忘门）

LSTM 的第一步是决定我们要从单元状态中丢弃哪些信息。此决定由称为“忘记门层”的 S 形结肠层做出。它着眼于ht−1和xt，并输出一个介于0和1对于单元格状态中的每个数字Ct−1.一个1代表“完全保留这个”，而一个0代表“完全摆脱这个”。

让我们回到我们的示例，该语言模型试图根据之前的所有单词预测下一个单词。在这样的问题中，单元格状态可能包括当前主语的性别，以便可以使用正确的代词。当我们看到一个新主题时，我们想忘记旧主题的性别。

![img](https://colah.github.io/posts/2015-08-Understanding-LSTMs/img/LSTM3-focus-f.png)

**偏置向量**：起到平移作用，以便调整神经元的输出。在计算激活函数之前，偏置向量会被加到神经元的加权输入和上。这使得神经元即使在所有输入值为零时仍然能够输出非零值，从而增加模型的灵活性和表达能力

##### 输入门和候选细胞状态：

下一步是确定我们要在单元格状态中存储哪些新信息。这有两个部分。首先，一个称为“输入门层”的 S 形层决定了我们要更新的值。接下来，tanh 层创建一个新的候选值向量，*C*̃*t*，可以添加到状态中。在下一步中，我们将这两者结合起来，以创建对状态的更新。

在我们的语言模型示例中，我们希望将新主题的性别添加到单元格状态中，以替换我们忘记的旧主题。

![img](https://colah.github.io/posts/2015-08-Understanding-LSTMs/img/LSTM3-focus-i.png)

现在是时候更新旧的单元格状态了，Ct−1，进入新的单元格状态Ct.前面的步骤已经决定了要做什么，我们只需要实际去做。

我们将旧状态乘以ft，忘记了我们之前决定忘记的事情。然后我们添加it*c~t.这是新的候选值，根据我们决定更新每个状态值的程度进行缩放。

在语言模型的情况下，正如我们在前面的步骤中决定的那样，我们实际上会删除有关旧主题性别的信息并添加新信息。

![img](https://colah.github.io/posts/2015-08-Understanding-LSTMs/img/LSTM3-focus-C.png)

最后，我们需要决定要输出什么。此输出将基于我们的单元格状态，但将是过滤版本。首先，我们运行一个 sigmoid 层，它决定了我们要输出的单元状态的哪些部分。然后，我们让细胞状态通过tanh（将值推至两者之间−1和1），并将其乘以 sigmoid 门的输出，这样我们就只输出我们决定的部分。

对于语言模型示例，由于它刚刚看到一个主语，因此它可能希望输出与动词相关的信息，以防接下来会发生什么。例如，它可能会输出主语是单数还是复数，这样我们就知道如果接下来是的话，动词应该变位为什么形式。

![img](https://colah.github.io/posts/2015-08-Understanding-LSTMs/img/LSTM3-focus-o.png)
$$
xt：输入数据（在时间步 t 时的输入向量）。\\

h_{t-1}：前一个时间步的隐藏状态。\\

C_{t-1}：前一个时间步的细胞状态（cell state）。\\

f_t：遗忘门（forget gate），决定当前细胞状态中哪些部分需要丢弃。\\

i_t：输入门（input gate），决定将多少新的信息写入细胞状态。\\

\tilde{C}_t：候选细胞状态（candidate cell state），通过 tanh 激活函数生成。\\

o_t：输出门（output gate），决定输出什么样的隐藏状态。\\

\sigma：表示 sigmoid 激活函数。\\

⁡\tanh：表示 tanh 激活函数。\\

C_t：当前时间步的细胞状态。\\

h_t：当前时间步的隐藏状态。\\
$$
**此外还有GRU等一系列变体，在这里不进行详述。**
