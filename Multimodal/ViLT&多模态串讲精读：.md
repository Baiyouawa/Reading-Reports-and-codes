## ViLT&多模态串讲精读：

**ViLT提出了极其简单的做多模态框架的学习结构，把模态的特征提取做到了极小化，把主要的计算量放在了后面的模态融合上，提高了模型推理速度。**

（图片1）：

在之前的任务中，对于文本我们直接进入一个Embedding嵌入矩阵得到相应的离散输出，进入transformer。但是对于图片领域，我们是相当于用目标检测任务，分成一个个离散的区域特征块，进入到transformer里做融合。**但是使用目标检测的视觉任务在时间上耗费较多。**

### 一：摘要：

 视觉-语言预训练 (VLP) 已在各种视觉与语言的联合下游任务中提升了性能。**目前的 VLP 方法严重依赖于图像特征提取过程，其中大多数涉及区域监督（如目标检测）和卷积架构（如 ResNet**）。虽然在文献中常被忽略，但我们发现这在以下两方面存在问题：**(1) 效率/速度**：仅仅提取输入特征就需要比多模态交互步骤更多的计算；**(2) 表达能力**：受限于视觉嵌入器及其预定义的视觉词汇表的表达能力上限。在本文中，我们提出了一个极简的 VLP 模型，称为 Vision-and-Language Transformer (ViLT)。该模型在视觉输入的处理中去掉了卷积，简化为与文本输入相同的处理方式。我们展示了 ViLT 的速度比之前的 VLP 模型快了数十倍，但在下游任务中的表现仍具竞争力或更优。我们的代码和预训练权重可在以下地址获取：https://github.com/dandelin/vilt。

### 二：引言：  

**预训练和微调方案已经扩展到视觉和语言的联合领域**，诞生了视觉-语言预训练（VLP）模型的类别 (Lu et al., 2019; Chen et al., 2019; Su et al., 2019; Li et al., 2019; Tan & Bansal, 2019; Li et al., 2020a; Lu et al., 2020; Cho et al., 2020; Qi et al., 2020; Zhou et al., 2020; Huang et al., 2020; Li et al., 2020b; Gan et al., 2020; Yu et al., 2020; Zhang et al., 2021)。这些模型通过图像文本匹配和掩码语言建模目标对图像及其对应的描述进行预训练，然后在视觉-语言下游任务中进行微调，输入数据涉及两种模态。

为了将图像输入到 VLP 模型中，图像像素需要与语言标记**一起初步嵌入为密集形式**。自 Krizhevsky 等人 (2012) 的开创性工作以来，**深度卷积网络被认为是视觉嵌入步骤的关键**。大多数 VLP 模型使用在 Visual Genome 数据集 (Krishna et al., 2017) 上预训练的目标检测器，后者注释了 1,600 个目标类和 400 个属性类 (如 Anderson et al., 2018 所述)。PixelBERT (Huang et al., 2020) 是这一趋势的一个例外，它使用在 ImageNet 分类 (Russakovsky et al., 2015) 上预训练的 ResNet 变体 (He et al., 2016; Xie et al., 2017) 嵌入像素，而不是使用目标检测模块。

**如何把离散型的像素转换为带有语义性的，离散型的特征。**

到目前为止，大多数 VLP 研究都集中在**通过增强视觉嵌入器的能力**（目标检测器）来提高性能。在学术实验中，重型视觉嵌入器的缺点往往被忽视，因为在训练时通常会预先缓存区域特征，以减轻特征提取的负担。然而，在实际应用中，这种限制仍然很明显，因为野外的查询必须经历缓慢的提取过程。

为此，我们**将注意力转向轻量级且快速的视觉输入嵌入**。最近的工作 (Dosovitskiy et al., 2020; Touvron et al., 2020) 表明，**使用简单的线性投影补丁在将其输入到 Transformer 之前足够有效地嵌入像素**。虽然 Transformer 在文本处理领域是坚实的主流 (Devlin et al., 2019)，但直到最近，Transformer (Vaswani et al., 2017) 才被用于图像处理。*我们推测*，VLP 模型中**用于模态交互的 Transformer 模块也可以代替卷积视觉嵌入器处理视觉特征，就像它处理文本特征一样**。

**从VIT得出灵感**

本文提出了 Vision-and-Language Transformer (ViLT)，该模型**以统一的方式处理两种模态**。它主要不同于之前的 VLP 模型，体现在其浅层、无卷积的像素级输入嵌入。**去除专用于视觉输入的深层嵌入器，通过设计显著减少了模型大小和运行时间**。图1显示了我们参数高效的模型比使用区域特征的 VLP 模型快数十倍，比使用网格特征的模型至少快四倍，同时在视觉-语言下游任务中表现相似或更好。

我们的主要贡献总结如下：
- ViLT 是迄今为止最简洁的视觉-语言模型架构，因为**它委托 Transformer 模块提取和处理视觉特征，代替单独的深层视觉嵌入器**。这一设计本质上带来了显著的运行时间和参数效率。
- 首次，我们在没有使用区域特征或深层卷积视觉嵌入器的情况下，在视觉-语言任务上取得了有竞争力的表现。
- 此外，我们首次通过实验证明，在 VLP 训练方案中前所未有的**全词掩码和图像增强**进一步提升了下游性能。

### 三：背景知识：

### 2. 背景
#### 2.1. 视觉与语言模型的分类
我们提出了一个基于两个点的视觉与语言模型分类方法：(1) **两种模态在专用参数和/或计算方面是否具有相同的表达水平**；(2) **两种模态是否在深层网络中进行交互。这些点的组合形成了图2中的四种原型**。

视觉语义嵌入 (VSE) 模型，如 VSE++ (Faghri et al., 2017) 和 SCAN (Lee et al., 2018) 属于图2a。它们为图像和文本使用独立的嵌入器，前者要复杂得多。然后，它们通过简单的点积或浅层注意力层来表示两种模态的嵌入特征的相似性。

CLIP (Radford et al., 2021) 属于图2b，因为它为每个模态使用了独立但计算量相当大的 Transformer 嵌入器。图像向量和文本向量之间的交互仍然是浅层的（点积）。尽管 CLIP 在图像到文本检索上的零样本表现令人瞩目，但我们在其他视觉与语言下游任务上未能观察到同样的表现。例如，使用 CLIP 的池化视觉和文本向量的点积作为多模态表示，在 NLVR2 (Suhr et al., 2018) 上微调 MLP 头得到了 50.99 ± 0.38 的开发集准确率（使用了三个不同的随机种子）；由于随机水平的准确率为 0.5，我们得出结论，这些表示无法学习此任务。这也符合 Suhr et al. (2018) 的发现，即所有简单融合多模态表示的模型都未能学习 NLVR2。

这一结果支持了我们的推测，即即使是高性能的单模态嵌入器的输出的简单融合也可能不足以学习复杂的视觉与语言任务，因此需要更严格的模态间交互方案。

与浅层交互的模型不同，最近的属于图2c的 VLP 模型使用深度 Transformer 来建模图像和文本特征的交互。然而，除了交互模块外，卷积网络仍然参与了图像特征的提取和嵌入，这占据了大部分计算量，如图1所示。基于调制的视觉与语言模型 (Perez et al., 2018; Nguyen et al., 2020) 也属于图2c，其中视觉 CNN 干作为视觉嵌入器，RNNs 生成文本嵌入器的调制参数，调制 CNNs 用于模态交互。

我们提出的 ViLT 是图2d类型的第一个模型，其中原始像素的嵌入层与文本标记一样浅且计算量小。该架构因此将大部分计算集中在建模模态交互上。

#### 2.2. 模态交互方案
当代 VLP 模型的核心是 Transformers。它们将视觉和文本嵌入序列作为输入，在层中建模模态间和可选的模态内交互，然后输出上下文化的特征序列。

Bugliarello et al. (2020) 将交互方案分为两类：(1) 单流方法（如 VisualBERT (Li et al., 2019)、UNITER (Chen et al., 2019)），**其中层共同作用于图像和文本输入的串联**；(2) 双流方法（如 ViLBERT (Lu et al., 2019)、LXMERT (Tan & Bansal, 2019)），其中两种模态在输入级别不被串联。**我们为我们的交互 Transformer 模块采用了单流方法，因为双流方法会引入额外的参数**。

#### 2.3. 视觉嵌入方案
虽然所有性能优异的 VLP 模型共享相同的**文本嵌入器——来自预训练 BERT 的分词器**、类似 BERT 的词汇和位置嵌入——但它们在视觉嵌入器上有所不同。然而，在大多数（如果不是全部）情况下，视觉嵌入是现有 VLP 模型的瓶颈。我们**通过引入补丁投**影，而不是使用需要大量提取模块的区域或网格特征，来简化这一步骤。

**区域特征**。VLP 模型主要使用区域特征，也称为自下而上的特征 (Anderson et al., 2018)。这些特征是通过像 Faster R-CNN (Ren et al., 2016) 这样的现成目标检测器获得的。

生成区域特征的一般流程如下。首先，区域提议网络 (RPN) 根据从 CNN 主干池化的网格特征提议感兴趣区域 (RoI)。然后，非最大抑制 (NMS) 将 RoI 的数量减少到几千个。在通过 RoI Align (He et al., 2017) 等操作进行池化后，RoI 会通过 RoI 头部并成为区域特征。NMS 再次应用于每个类别，最终将特征数量减少到不到一百个。

上述过程涉及几个影响性能和运行时间的因素：**主干、NMS 的样式、RoI 头。之前的工作在控制这些因素上较为宽松，表7列出了它们不同的选择**。

- **主干**：ResNet-101 (Lu et al., 2019; Tan & Bansal, 2019; Su et al., 2019) 和 ResNext-152 (Li et al., 2019; 2020a; Zhang et al., 2021) 是两种常用的主干。
- **NMS**：NMS 通常按类别执行。对每个类别应用 NMS 会成为运行时间的主要瓶颈，尤其是在具有大量类别时，例如 VG 数据集中的 1.6K 类 (Jiang et al., 2020)。最近引入了类别无关的 NMS 来解决这一问题 (Zhang et al., 2021)。
- **RoI 头**：最初使用的是 C4 头 (Anderson et al., 2018)。后来引入了 FPN-MLP 头 (Jiang et al., 2018)。由于头部针对每个 RoI 运行，它们构成了相当大的运行时间负担。

然而，即使是轻量级的目标检测器也不太可能比主干或单层卷积快。冻结视觉主干并提前缓存区域特征只能在训练时有帮助，而不是在推理时，更不用说它可能会影响性能。

**网格特征**。除了检测器头外，卷积神经网络（如 ResNet）的输出特征网格也可以用作视觉与语言预训练的视觉特征。VQA 专用模型首次提出了直接使用网格特征 (Jiang et al., 2020; Nguyen et al., 2020)，主要是为了避免使用极其缓慢的区域选择操作。

X-LXMERT (Cho et al., 2020) 通过将区域提议固定在网格上而不是从区域提议网络中提出，重新审视了网格特征。然而，它们的特征缓存排除了对主干的进一步调整。

Pixel-BERT 是唯一一个将 VG 预训练的目标检测器替换为 ImageNet 分类预训练的 ResNet 变体主干的 VLP 模型。与区域特征 VLP 模型中冻结的检测器不同，Pixel-BERT 的主干在视觉与语言预训练期间进行了调整。使用 ResNet-50 的 Pixel-BERT 的下游性能低于基于区域特征的 VLP 模型，但通过使用更复杂的 ResNeXt-152，与其他竞争对手的性能相当。

**然而，我们认为网格特征并不是首选，因为深度 CNN 仍然非常昂贵**，如图1所示，它们占据了整个计算量的一大部分。

**补丁投影**。为了尽量减少开销，我们采用了最简单的视觉嵌入方案：**对图像补丁进行线性投影。补丁投影嵌入由 ViT (Dosovitskiy et al., 2020) 引入，用于图像分类任务**。补丁投影将视觉嵌入步骤大大简化到文本嵌入的水平，后者也由简单的投影（查找）操作组成。我们使用 32 × 32 的补丁投影，只需要 2.4M 参数。这与复杂的 ResNe(X)t 主干和检测组件形成了鲜明对比。其运行时间也如图1所示，可忽略不计。我们将在第4.6节中进行详细的运行时间分析。
