## Transformer学习（二）：Bert：

#### 论文网址：[Bert](https://arxiv.org/abs/1810.04805)

#### 视频网址：[论文精读](【BERT 论文逐段精读【论文精读】】https://www.bilibili.com/video/BV1PL411M7eQ?vd_source=88664659bdda4409e78f614f5f213ce8)

### 一.论文：

#### 1.标题：

**Bert:是一个用于语言理解预训练的双向transformer模型**

Bert的出现使得在NLP领域，可以像类似于CNN一样，不用再为每一个特定的任务搭建新的神经网络模型，而是可以针对一个大的数据集训练出来模型应用在其他的NLP任务上可以简化训练，提升性能。

------

#### 2.摘要：

我们引入了一种新的语言表示模型，称为BERT，代表双向编码器表示的转换器。与最近的语言表示模型（Peters et al., 2018a; Radford et al., 2018）不同，BERT旨在通过在所有层中联合条件化左右上下文，**从未标注的文本中预训练深度双向表示**。结果，预训练的BERT模型**只需一个额外的输出层**即可微调，从而为广泛的任务（如问答和语言推理）创建最先进的模型，而**无需进行大量的任务特定架构修改**。

BERT在概念上简单且在实证上强大。它在十一项自然语言处理任务中取得了新的最先进的结果，包括将GLUE评分推升至80.5%（绝对提高7.7个百分点3.......），将MultiNLI准确率提高到86.7%（绝对提高4.6个百分点），SQuAD v1.1问答测试F1分数提高到93.2（绝对提高1.5个百分点）以及SQuAD v2.0测试F1分数提高到83.1（绝对提高5.1个百分点）。

**解读：相对比而言，GPT是单向的，靠左边的信息来预测右侧，而Bert是双向的（左右上下文进行了解），Elmo是基于RNN的架构，而Bert是基于transformer。Elmo在进行一些下游任务需要对框架进行调整，但是Bert相比更加简单。在很多任务上，不需要进行较大的调整，只需要进行一定的微调。**

------

#### 2.导言：

语言模型的**预训练**【1】已被证明对改进许多自然语言处理任务有效（Dai和Le，2015；Peters等，2018a；Radford等，2018；Howard和Ruder，2018）。这些任务包括句子级任务，如自然语言推理（Bowman等，2015；Williams等，2018）和释义（Dolan和Brockett，2005），旨在通过整体分析句子来预测句子之间的关系，以及如命名实体识别和问答等词级任务，模型需要在词级别生成细粒度输出（Tjong Kim Sang和De Meulder，2003；Rajpurkar等，2016）。

目前，有两种将预训练语言表示应用于下游任务的策略：**基于特征和微调**【2】。基于特征的方法，如ELMo（Peters等，2018a），使用任务特定架构，将预训练表示作为附加特征。微调的方法，如生成预训练转换器（OpenAI GPT）（Radford等，2018），引入最少的任务特定参数，通过简单地微调所有预训练参数在下游任务上进行训练。**两种方法在预训练期间共享相同的目标函数，使用单向语言模型学习通用语言表示**。

我们认为，当前技术限制了预训练表示的能力，特别是对微调方法。主要的限制是**标准语言模型是单向的**，这限制了在预训练期间可以使用的架构选择。例如，在OpenAI GPT中，作者使用了从左到右的架构，其中每个令牌只能在转换器的自注意层中关注以前的令牌。这种限制对于句子级任务来说是不理想的，当将微调方法应用于如**问答等词级任务**时，可能会非常有害，因为在这些任务中，结合双向上下文是至关重要的。

在本文中，我们通过提出BERT（**双向编码器表示的转换器**）改进了基于微调的方法。BERT通过使用“掩码语言模型”（MLM）预训练目标缓解了之前提到的单向性约束，该目标受Cloze任务（Taylor，1953）的启发。掩码语言模型**随机掩盖输入中的一些令牌**，目标是仅根据其上下文预测被掩盖单词的原始词汇ID。与从左到右的语言模型预训练不同，MLM目标使表示能够融合左右上下文，使我们能够预训练深度双向转换器。除了掩码语言模型，我们还使用“下一个句子预测”任务，共同预训练文本对表示。我们论文 的贡献如下：

- 我们展示了双向预训练对语言表示的重要性。与使用单向语言模型进行预训练的Radford等（2018）不同，BERT使用掩码语言模型来实现预训练的深度双向表示。这也不同于Peters等（2018a），后者使用独立训练的从左到右和从右到左语言模型的浅层拼接。
- 我们证明了预训练表示减少了对许多高度工程化的任务特定架构的需求。BERT是第一个基于微调的表示模型，在大量句子级和词级任务上实现了最先进的性能，超过了许多任务特定架构。
- BERT推动了十一项自然语言处理任务的最先进水平。代码和预训练模型可在https://github.com/google-research/bert找到。

【1】：**预训练**：预训练是指在大规模无标签数据集上训练模型，以学习通用的特征表示。这个过程不涉及特定任务，只是为了让模型具备一定的语言理解能力，进而使用在别的任务上，简化提升性能。

【2】：**基于特征和微调**：这种方法使用预训练模型生成的特征作为输入，结合特定任务的模型进行训练。预训练模型的参数在下游任务中保持不变

​        微调是指在预训练模型的基础上，对整个模型（包括预训练的参数）进行进一步训练，使其适应特定的下游任务，通过在特定任务上微调模型，可以充分利用预训练模型的知识，同时让模型更好地适应特定任务的数据分布和要求。

**解读：ELMo利用Rnn架构，将预训练表示作为额外的特征，跟输入一起进入模型；或者是类似于GPT，预训练好的参数，会在新的数据集上进行微调，权重参数。但这两个途径都是使用单向的语言模型，是共同的目标函数。（预测）**

------

#### 3.结论：

由于使用语言模型进行迁移学习的最新实证改进表明，丰富的**无监督预训练**【3】是许多语言理解系统的一个重要组成部分。特别是，这些结果使得即使是资源较少的任务也能从深度单向架构中受益。我们的主要贡献是进一步将这些发现推广到深度双向架构，使得同一个预训练模型能够成功应对广泛的自然语言处理任务。

**解读：将Elmo的双向性和Transformer结合起来，形成了Bert。**

【3】：**无监督预训练**：无监督预训练是指在没有标签的数据上训练模型，通常使用语言建模等任务来学习语言表示。与有监督学习不同，无监督预训练不需要手工标注的数据，通过无监督预训练，模型可以从大量未标注的数据中学习有用的特征表示，为后续的监督学习任务打下基础。

------

#### 4.相关工作：

#### 4.1无监督的基于特征的方法【补充】
学习广泛适用的词表示一直是数十年来的一个活跃研究领域，包括非神经方法（Brown et al., 1992; Ando and Zhang, 2005; Blitzer et al., 2006）和神经方法（Mikolov et al., 2013; Pennington et al., 2014）。预训练的词嵌入是现代自然语言处理系统的重要组成部分，与从头学习的嵌入相比，提供了显著的改进（Turian et al., 2010）。为了预训练词嵌入向量，已经使用了从左到右的语言模型目标（Mnih and Hinton, 2009），以及在左和右上下文中区分正确词与错误词的目标（Mikolov et al., 2013）。

这些方法已被推广到更粗粒度的表示，例如句子嵌入（Kiros et al., 2015; Logeswaran and Lee, 2018）或段落嵌入（Le and Mikolov, 2014）。为了训练句子表示，以前的工作使用了对候选下一句进行排序的目标（Jernite et al., 2017; Logeswaran and Lee, 2018），给定前一句表示的情况下从左到右生成下一句词的目标（Kiros et al., 2015），或去噪自动编码器衍生的目标（Hill et al., 2016）。

ELMo及其前身（Peters et al., 2017, 2018a）在不同维度上推广了传统的词嵌入研究。它们从左到右和从右到左的语言模型中提取上下文敏感的特征。每个标记的上下文表示是左右两种表示的拼接。当将上下文词嵌入与现有的特定任务架构集成时，ELMo在多个主要的自然语言处理基准上提升了技术水平（Peters et al., 2018a），包括问答（Rajpurkar et al., 2016）、情感分析（Socher et al., 2013）和命名实体识别（Tjong Kim Sang and De Meulder, 2003）。Melamud et al.（2016）提出通过任务来预测使用LSTM从左右上下文中学习单个词的上下文表示。与ELMo类似，他们的模型是基于特征的，并不是深度双向的。Fedus et al.（2018）表明完形填空任务可以用于提高文本生成模型的鲁棒性。

#### 4.2:无监督的基于微调的方法【补充】

与基于特征的方法类似，这一方向的最早工作仅从未标记的文本中预训练词嵌入参数（Collobert 和 Weston, 2008）。最近，从未标记的文本中预训练句子或文档编码器，并对其进行微调以适应监督的下游任务（Dai 和 Le, 2015；Howard 和 Ruder, 2018；Radford 等, 2018）。这些方法的优势在于，只有少量的参数需要从头开始学习。至少部分由于这一优势，OpenAI GPT（Radford 等, 2018）在GLUE基准测试的许多句子级任务上达到了之前的最先进成果（Wang 等, 2018a）。从左到右的语言模型和自编码器目标已被用于预训练这样的模型（Howard 和 Ruder, 2018；Radford 等, 2018；Dai 和 Le, 2015）。

#### 4.3 从监督数据中进行迁移学习【补充】

也有一些研究表明，从具有大数据集的监督任务中进行有效迁移是可行的，例如自然语言推理 (Conneau et al., 2017) 和机器翻译 (McCann et al., 2017)。计算机视觉研究也展示了从大型预训练模型中进行迁移学习的重要性，一个有效的方法是对使用 ImageNet 预训练的模型进行微调 (Deng et al., 2009; Yosinski et al., 2014)。

------

#### 5.模型：**Bert**

本节中，我们介绍了BERT及其详细实现。我们的框架包括两个步骤：**预训练和微调**。在预训练过程中，模型在不同的预训练任务上使用无标签数据进行训练。在微调过程中，BERT模型首先使用预训练的参数进行初始化，然后使用下游任务的有标签数据**对所有参数进行微调**。尽管所有任务都使用相同的预训练参数进行初始化，但每个下游任务都有单独的微调模型。图1中的问答示例将作为本节的持续示例。

BERT的一个显著特点是其在不同任务中的统一架构。预训练架构和最终的下游架构之间的差异极小。

**解读：有趣的是在大量的无标号的训练集上进行训练，要比在小型的有标号的数据集上训练效果更好，同样的不只是NLP领域，在CV等领域也有很大的表现。**

------

#### 模型架构：

![](F:\科研文档\Transformer\Bert\bert.png)

BERT的模型架构是一个基于原始实现的**多层双向Transformer编码器**，该实现描述于Vaswani et al. (2017) 并在tensor2tensor库中发布。由于Transformer的使用已变得普遍，并且我们的实现与原始实现几乎相同，我们将省略对模型架构的详尽背景描述，并将读者参考至Vaswani et al. (2017) 以及诸如“The Annotated Transformer”等优秀指南。

在本文中，我们将层数（即Transformer块数）表示为L，隐藏层大小表示为H，自注意力头数表示为A。我们主要报告两种模型大小的结果：BERTBASE (L=12, H=768, A=12, 总参数量=110M) 和 BERTLARGE (L=24, H=1024, A=16, 总参数量=340M)。  

为了比较，选择了与OpenAI GPT相同模型大小的BERTBASE。然而，关键的是，BERT Transformer使用双向自注意力，而GPT Transformer使用受限自注意力，其中每个令牌只能关注其左侧的上下文。

**解读：参数计算：**

**可学习的参数来源：嵌入层30k×H，transformer块的L×H^2×12**

**嵌入层:**输入是词的字典大小30k,输出是H

参数:30k(字典大小)×H(隐藏层大小)

嵌入层的输出会进入Transformer模块

**Transformer块:**(H^2×12) = 自注意力机制(H^2×4)+MLP(H^2×8)

**自注意力机制**本身无可学习参数;多头自注意力机制要对QKV做投影,每一次投影维度 = 64 -->A×64 = H

每一个QKV都有自己的投影矩阵,合并每个head的投影矩阵 --> QKV分别的H×H矩阵

得到输出后还会有一次H×H的投影

所以自注意力机制里可学习参数 = H^2×4

**MLP的两个全连接层**

第一个连接层输入H,输出4×H;第二个输入4×H,输出是H

每个参数矩阵大小H×4H

也就是H^2×8



#### 输入/输出表示

为了使 BERT 能够处理多种下游任务，我们的输入表示能够明确地表示单个句子和句子对（例如，问题和答案）在一个 token 序列中。在这项工作中，“句子”可以是任意连续文本的跨度，而不一定是实际的语言句子。“序列”指的是 BERT 的输入 token 序列，这可以是单个句子或两个句子打包在一起。

我们使用 WordPiece 嵌入（Wu 等，2016）和一个 30,000 token 的词汇表。每个序列的第一个 token 始终是一个特殊的分类 token（[CLS]）。与此 token 对应的最终隐藏状态用于分类任务的聚合序列表示。句子对被打包在一个序列中。我们通过两种方式区分句子。首先，我们用一个特殊的 token（[SEP]）将它们分开。其次，我们为每个 token 添加一个学习到的嵌入，指示它属于句子 A 还是句子 B。如图 1 所示，我们将输入嵌入表示为 E，将特殊 [CLS] token 的最终隐藏向量表示为 C ∈ R^H，将第 i 个输入 token 的最终隐藏向量表示为 Ti ∈ R^H。

对于给定的 token，其输入表示通过对相应的 token、段落和位置嵌入求和来构建。此构造的可视化可以在图 2 中看到。

![](F:\科研文档\Transformer\Bert\屏幕截图2024.08.07.png)

**解读：与Transformer不同的是,transformer是输入一个序列对,因为编码器和解码器架构,而在Bert上我们只有一个编码器,所以为了能处理两个句子,就把两个句子变成一个序列.WordPiece把词语切开,看词根子序列,保证了字典不会特别大.[SEP]标记和嵌入层学习来区分两个句子**

#### Bert预训练：

与 Peters 等人 (2018a) 和 Radford 等人 (2018) 不同，我们没有使用传统的从左到右或从右到左的语言模型来预训练 BERT。相反，我们使用两个无监督任务来预训练 BERT，如本节所述。这一步在图 1 的左边部分展示。

#### 任务1：Masked LM

直观地说，可以相信一个深度双向模型比一个从左到右的模型或一个从左到右和从右到左的模型的浅层连接更强大。不幸的是，标准的条件语言模型只能从左到右或从右到左训练，因为双向条件会让每个词间接地“看到自己”，模型可以在多层上下文中轻松预测目标词。

为了训练一个深度双向表示，我们随机屏蔽输入词的某些百分比，然后预测这些屏蔽的词。我们将此过程称为“Masked LM” (MLM)，尽管在文献中通常称为克洛兹任务（Cloze task）（Taylor，1953）。在这种情况下，与屏蔽词相对应的最终隐藏向量被输入到词汇表上的输出 softmax 中，就像在标准 LM 中一样。在我们所有的实验中，我们随机屏蔽每个序列中 15% 的 WordPiece 词。与去噪自动编码器 (Vincent 等人，2008) 不同，我们只预测屏蔽的词而不是重构整个输入。

虽然这使我们能够获得一个双向预训练模型，**但缺点是我们在预训练和微调之间创建了不匹配**，因为 [MASK] 词在微调期间不会出现。为了解决这个问题，我们并不总是用实际的 [MASK] 词替换“屏蔽”词。训练数据生成器随机选择 15% 的词位置进行预测。如果选择了第 i 个词，我们用 (1) [MASK] 词替换 i 词 80% 的时间 (2) 随机词替换 i 词 10% 的时间 (3) 保持 i 词不变 10% 的时间。然后，用 **Ti 预测**原始词，并使用交叉熵损失来进行训练。我们在附录 C.2 中比较了这种过程的变体。

#### 任务2：下一句预测 (NSP)

许多重要的下游任务（如问答 (QA) 和自然语言推理 (NLI)）都基于理解两个句子之间的关系，而这并不是语言模型直接捕捉的。为了训练一个理解句子关系的模型，我们对一个可以从任何单语语料库中简单生成的二值化下一句预测任务进行预训练。具体来说，在为每个预训练样本选择句子 A 和 B 时，50% 的时间 B 是实际跟随 A 的下一个句子（标记为 IsNext），50% 的时间它是语料库中的随机句子（标记为 NotNext）。如图 1 所示，C 用于下一句预测 (NSP)。尽管其简单性，我们在第 5.1 节中展示了针对该任务进行预训练对 QA 和 NLI 非常有利。NSP 任务与 Jernite 等人 (2017) 和 Logeswaran 和 Lee (2018) 中使用的表示学习目标密切相关。然而，在以前的工作中，只有句子嵌入被转移到下游任务，而 BERT 将所有参数转移到初始化终端任务模型参数。

预训练数据
预训练过程在很大程度上遵循现有的语言模型预训练文献。对于预训练语料库，我们使用 BooksCorpus（8 亿字）（Zhu 等人，2015）和英文维基百科（25 亿字）。对于维基百科，我们只提取文本段落，忽略列表、表格和标题。使用文档级语料库而不是诸如 Billion Word Benchmark（Chelba 等人，2013）之类的洗牌句子级语料库来提取长的连续序列是至关重要的。



#### 微调：

微调相对简单，因为 Transformer 中的自注意力机制允许 BERT 通过交换适当的输入和输出来建模许多下游任务——无论它们涉及单一文本还是文本对。对于涉及文本对的应用，常见的模式是独立编码文本对，然后应用双向交叉注意力，例如 Parikh 等人 (2016) 和 Seo 等人 (2017)。而 BERT 使用自注意力机制来统一这两个阶段，因为使用自注意力编码连接的文本对有效地在两个句子之间包含了双向交叉注意力。

对于每个任务，我们只需将任务特定的输入和输出插入 BERT，并端到端微调所有参数。在输入方面，预训练中的句子 A 和句子 B 类似于 (1) 在释义中的句子对，(2) 在蕴含关系中的假设-前提对，(3) 在问答中的问题-段落对，以及 (4) 在文本分类或序列标记中的退化文本-空对。在输出方面，令牌表示被输入到用于令牌级任务的输出层，例如序列标记或问答，而 [CLS] 表示被输入到用于分类的输出层，例如蕴含或情感分析。

与预训练相比，微调相对便宜。本文c中的所有结果最多可以在单个 Cloud TPU 上一小时内复制，或者在 GPU 上几个小时内复制，从完全相同的预训练模型开始。我们在第 4 节的相应小节中描述了任务特定的细节。更多细节可以在附录 A.5 中找到。



### 二：补充知识：

#### 一：Elmo模型：无监督的基于特征的方法

**ELMo (Embeddings from Language Models)** 是由Allen Institute for AI提出的一种深度上下文化的词表示模型，它能够捕捉到单词在句子中的语法和语义信息。

#### 1. 特点

- **无监督训练:** ELMo通过大量未标注的文本进行训练，使用双向语言模型（BiLM）来学习上下文信息。
- **基于特征:** ELMo生成的词嵌入（word embeddings）可以作为特征输入到下游的自然语言处理任务中，比如命名实体识别、情感分析等。

#### 2. 训练方法

- ELMo使用双向LSTM网络从左到右和从右到左分别训练语言模型，然后将两者的输出进行拼接。
- 每个单词的表示不仅依赖于该单词本身，还依赖于它在句子中的上下文。

#### 3. 应用

- ELMo嵌入可以用于增强现有的NLP模型，在多个NLP基准测试中显著提高了性能，包括SQuAD（Stanford Question Answering Dataset）、情感分析和命名实体识别等任务。

**进一步阅读:**

- ELMo博客 (Allen Institute for AI)
- [ELMo原始论文](https://arxiv.org/abs/1802.05365)

#### 二：GPT模型：:无监督的基于微调的方法

**GPT (Generative Pre-trained Transformer)** 是由OpenAI提出的一种生成式预训练模型，通过在大规模未标注文本上进行训练来生成语言表示。

#### 1. 特点

- **无监督训练:** GPT使用未标注的文本数据进行预训练，采用左到右的语言模型来预测下一个单词。
- **基于微调:** 预训练后的模型通过添加少量任务特定的参数，并在下游任务上进行微调，从而适应不同的任务。

#### 2. 训练方法

- GPT使用Transformer架构进行训练，专注于生成式任务，即给定前面的文本来预测后续的文本。
- 预训练完成后，模型会根据具体的任务进行微调，通过添加任务特定的层来进行适配。

#### 3. 应用

- GPT在多个NLP任务中取得了优异的表现，包括文本生成、翻译、问答系统等。
- 它能够通过微调在下游任务上取得很好的效果，因为预训练阶段已经捕捉了大量的语言信息。

**进一步阅读:**

- [GPT-2博客 (OpenAI)](https://openai.com/blog/better-language-models/)
- [GPT原始论文](https://cdn.openai.com/better-language-models/language_models_are_unsupervised_multitask_learners.pdf)

#### 三：迁移学习：

**迁移学习** 是将一个领域的知识应用到另一个领域的方法。它在NLP和计算机视觉等领域显示了显著的效果。

#### 1. 特点

- **监督和无监督:** 迁移学习可以通过大规模监督数据进行预训练，也可以通过无监督方式进行预训练。
- **应用广泛:** 迁移学习不仅可以应用于NLP任务，还可以应用于计算机视觉等其他领域。

#### 2. 方法

- **监督迁移学习:** 例如在ImageNet上预训练的图像分类模型，然后在特定的图像分类任务上进行微调。
- **无监督迁移学习:** 例如在大规模未标注文本上进行语言模型预训练，然后在具体的NLP任务上进行微调。

#### 3. 应用

- 在自然语言处理方面，BERT等模型通过在大规模文本数据上进行预训练，然后在具体的任务上进行微调，取得了非常好的效果。
- 在计算机视觉方面，通过在ImageNet上预训练模型，然后在具体的图像识别任务上进行微调，也显示了非常好的效果。

**进一步阅读:**

- 迁移学习博客 (towardsdatascience)
- 迁移学习在计算机视觉中的应用 (知乎)
